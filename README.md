# Vision Transformer å’Œ MLP-Mixer æž¶æž„

åœ¨æœ¬ä»“åº“ä¸­ï¼Œæˆ‘ä»¬å‘å¸ƒäº†è¿™äº›è®ºæ–‡ä¸­æ‰€ä½¿ç”¨çš„æ¨¡åž‹ã€‚

- (ViT) [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
- (MLP-Mixer) [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601)
- [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/abs/2106.10270)
- [When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations](https://arxiv.org/abs/2106.01548)
- (LiT) [LiT: Zero-Shot Transfer with Locked-image text Tuning](https://arxiv.org/abs/2111.07991)
- [Surrogate Gap Minimization Improves Sharpness-Aware Training](https://arxiv.org/abs/2203.08065)


è¿™äº›æ¨¡åž‹åœ¨ [ImageNet](http://www.image-net.org/) å’Œ [ImageNet-21k](http://www.image-net.org/) æ•°æ®é›†ä¸Šè¿›è¡Œäº†é¢„è®­ç»ƒã€‚

ä½¿ç”¨ [JAX](https://jax.readthedocs.io) å’Œ [Flax](http://flax.readthedocs.io) æ¡†æž¶ç¼–å†™çš„æºä»£ç ï¼Œå¹¶ä¸”åœ¨å·²æœ‰é¢„è®­ç»ƒæ¨¡åž‹çš„åŸºç¡€ä¸Šç»§ç»­å¾®è°ƒè®­ç»ƒï¼Œä»¥é€‚é…æ–°çš„ä»»åŠ¡æˆ–æ•°æ®é›†ã€‚



è¿™äº›æ¨¡åž‹æœ€åˆæ˜¯åœ¨ä»¥ä¸‹ä»£ç åº“ä¸­è®­ç»ƒçš„ï¼šðŸ‘‰ https://github.com/google-research/big_vision/

åœ¨é‚£é‡Œï¼Œä½ å¯ä»¥æ‰¾åˆ°æ›´é«˜çº§çš„ä»£ç ï¼ˆä¾‹å¦‚ å¤šä¸»æœºè®­ç»ƒï¼ˆmulti-host trainingï¼‰ï¼‰ï¼Œä»¥åŠä¸€äº›æœ€åˆçš„è®­ç»ƒè„šæœ¬ï¼Œä¾‹å¦‚ï¼š

configs/vit_i21k.py
ï¼šç”¨äºŽ é¢„è®­ç»ƒ ViTï¼ˆVision Transformerï¼‰æ¨¡åž‹ï¼›

configs/transfer.py
ï¼šç”¨äºŽ è¿ç§»å·²æœ‰æ¨¡åž‹ï¼ˆtransfer learningï¼‰ã€‚

ç›®å½•:

- [è§†è§‰Transformerå’ŒMLP-Mixeræž¶æž„](#vision-transformer-and-mlp-mixer-architectures)
	- [Colabåœ¨çº¿è¿è¡Œ](#Colab)
	- [å®‰è£…æ­¥éª¤](#installation)
	- [å¾®è°ƒæ¨¡åž‹](#fine-tuning-a-model)
	- [è§†è§‰Transformerï¼ˆViTï¼‰æ¨¡åž‹](#vision-transformer)
		- [å¯ç”¨çš„ViTæ¨¡åž‹](#available-vit-models)
		- [ViTçš„é¢„æœŸç»“æžœ](#expected-vit-results)
	- [MLP-Mixeræ¨¡åž‹](#mlp-mixer)
		- [å¯ç”¨çš„Mixeræ¨¡åž‹](#available-mixer-models)
		- [Mixerçš„é¢„æœŸç»“æžœ](#expected-mixer-results)
	- [LiTæ¨¡åž‹](#lit-models)
	- [äº‘ç«¯è¿è¡Œ](#running-on-cloud)
		- [åˆ›å»ºè™šæ‹Ÿæœº](#create-a-vm)
		- [é…ç½®è™šæ‹Ÿæœº](#setup-vm)
	- [å‚è€ƒæ–‡çŒ®BibTeXæ¡ç›®](#bibtex)
	- [å…è´£å£°æ˜Ž](#disclaimers)
	- [æ›´æ–°æ—¥å¿—](#changelog)


## Colab

ä»¥ä¸‹ä¸¤ä¸ª Colab ç¤ºä¾‹éƒ½å¯ä»¥åœ¨ GPU æˆ– TPUï¼ˆ8 æ ¸æ•°æ®å¹¶è¡Œï¼‰ çŽ¯å¢ƒä¸‹è¿è¡Œã€‚

ðŸ”¹ç¬¬ä¸€ä¸ª Colab

æ¼”ç¤ºäº† Vision Transformer (ViT) å’Œ MLP-Mixer çš„ JAX å®žçŽ°ä»£ç ã€‚

åœ¨è¿™ä¸ª Colab ä¸­ï¼Œä½ å¯ä»¥ï¼š

ç›´æŽ¥åœ¨ Colab ç•Œé¢ä¸­ç¼–è¾‘ä»“åº“ä¸­çš„æ–‡ä»¶ï¼›

é€šè¿‡å¸¦æ³¨é‡Šçš„ä»£ç å•å…ƒæ ¼ï¼ˆannotated cellsï¼‰é€æ­¥å­¦ä¹ ä»£ç é€»è¾‘ï¼›

äº¤äº’å¼åœ°æ“ä½œä¸Žå¯è§†åŒ–æ•°æ®ã€‚

https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax.ipynb


ðŸ”¹ç¬¬äºŒä¸ª Colab

è¯¥ç¤ºä¾‹ç”¨äºŽæŽ¢ç´¢è¶…è¿‡ 5 ä¸‡ä¸ª Vision Transformer ä¸Žæ··åˆæ¨¡åž‹ï¼ˆhybridï¼‰æ£€æŸ¥ç‚¹ï¼ˆcheckpointsï¼‰ï¼Œè¿™äº›æ¨¡åž‹æ˜¯è®ºæ–‡
ã€ŠHow to train your ViT? ...ã€‹
ä¸­ç”Ÿæˆå®žéªŒæ•°æ®æ‰€ç”¨çš„æ¨¡åž‹ã€‚

è¯¥ Colab åŒ…å«ä»¥ä¸‹åŠŸèƒ½ï¼š

æä¾› æ£€æŸ¥ç‚¹æµè§ˆä¸Žé€‰æ‹© çš„ä»£ç ï¼›

æ”¯æŒä½¿ç”¨æœ¬ä»“åº“ä¸­çš„ JAX ä»£ç  æˆ– PyTorch çš„ [`timm`] åº“è¿›è¡ŒæŽ¨ç†ï¼ˆtimm å¯ç›´æŽ¥åŠ è½½è¿™äº›æ¨¡åž‹ï¼‰ï¼›

éƒ¨åˆ†æ¨¡åž‹ä¹Ÿå·²ç›´æŽ¥å‘å¸ƒåœ¨ TensorFlow Hub ä¸Šï¼ˆç”± [Sayak Paul] æä¾›çš„å¤–éƒ¨è´¡çŒ®ï¼‰ï¼Œä¾‹å¦‚
[sayakpaul/collections/vision_transformer]


æ­¤å¤–ï¼Œè¯¥ Colab è¿˜æ”¯æŒï¼š

å¯¹è¿™äº›é¢„è®­ç»ƒæ£€æŸ¥ç‚¹è¿›è¡Œå¾®è°ƒï¼ˆfine-tuningï¼‰ï¼›

æ”¯æŒä»»æ„ tfds æ•°æ®é›† æˆ– ä½ è‡ªå·±çš„ JPEG å›¾åƒæ•°æ®é›†ï¼ˆå¯ç›´æŽ¥ä»Ž Google Drive è¯»å–ï¼‰ã€‚

https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax_augreg.ipynb



âš ï¸ æ³¨æ„äº‹é¡¹ï¼ˆæˆªè‡³ 2021 å¹´ 6 æœˆ 20 æ—¥ï¼‰

Google Colab å½“å‰ä»…æ”¯æŒå•ä¸ª GPUï¼ˆNVIDIA Tesla T4ï¼‰ï¼›

TPUï¼ˆTPUv2-8ï¼‰ ä¸Ž Colab è™šæ‹Ÿæœºæ˜¯é€šè¿‡ç½‘ç»œé—´æŽ¥è¿žæŽ¥çš„ï¼Œé€šä¿¡å»¶è¿Ÿè¾ƒé«˜ï¼Œå¯¼è‡´è®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼›

è‹¥ä½ çš„å¾®è°ƒä»»åŠ¡æ¶‰åŠå¤§é‡æ•°æ®ï¼Œå»ºè®®æ­å»ºç‹¬ç«‹æœåŠ¡å™¨æˆ–äº‘ç«¯å®žä¾‹ï¼›

å…·ä½“éƒ¨ç½²æ–¹å¼è¯¦è§ç« èŠ‚[Running on cloud](#running-on-cloud)

[`timm`]: https://github.com/rwightman/pytorch-image-models
[sayakpaul/collections/vision_transformer]: https://tfhub.dev/sayakpaul/collections/vision_transformer
[Sayak Paul]: https://github.com/sayakpaul



## Installation

`Python>=3.10` 

Install JAX and python dependencies by running:

```
# If using GPU:
pip install -r vit_jax/requirements.txt

# If using TPU:
pip install -r vit_jax/requirements-tpu.txt
```

å¯¹äºŽæ–°ç‰ˆçš„ [JAX](https://github.com/google/jax), è¯·æŒ‰ç…§è¯¥ä»“åº“ä¸­æä¾›çš„å®‰è£…è¯´æ˜Žè¿›è¡Œæ“ä½œã€‚

éœ€è¦æ³¨æ„çš„æ˜¯ï¼ŒCPUã€GPU å’Œ TPU çš„å®‰è£…æ­¥éª¤ç•¥æœ‰ä¸åŒã€‚

å®‰è£… [Flaxformer](https://github.com/google/flaxformer), åŒæ ·è¯·éµå¾ªå…¶å¯¹åº”ä»“åº“ä¸­çš„å®‰è£…è¯´æ˜Žã€‚

å¦‚éœ€äº†è§£æ›´å¤šè¯¦æƒ…ï¼Œè¯·å‚è€ƒä¸‹æ–‡çš„äº‘ç«¯è¿è¡Œéƒ¨åˆ† [Running on cloud](#running-on-cloud)



## Fine-tuning a model

ä½ å¯ä»¥åœ¨è‡ªå·±æ„Ÿå…´è¶£çš„æ•°æ®é›†ä¸Šå¯¹ä¸‹è½½çš„æ¨¡åž‹è¿›è¡Œå¾®è°ƒï¼ˆfine-tuningï¼‰ã€‚æ‰€æœ‰æ¨¡åž‹éƒ½ä½¿ç”¨ç›¸åŒçš„å‘½ä»¤è¡ŒæŽ¥å£ã€‚

ä¾‹å¦‚ï¼Œè¦**åœ¨ CIFAR-10 æ•°æ®é›†ä¸Šå¾®è°ƒä¸€ä¸ªåœ¨ ImageNet-21k ä¸Šé¢„è®­ç»ƒè¿‡çš„ ViT-B/16 æ¨¡åž‹**
ï¼ˆè¯·æ³¨æ„ï¼Œæˆ‘ä»¬åœ¨é…ç½®å‚æ•°ä¸­ä½¿ç”¨äº† b16,cifar10ï¼Œå¹¶é€šè¿‡ --config.pretrained_dir è®©ä»£ç ç›´æŽ¥ä»Ž GCS äº‘ç«¯å­˜å‚¨æ¡¶ è¯»å–æ¨¡åž‹ï¼Œè€Œä¸æ˜¯å…ˆä¸‹è½½åˆ°æœ¬åœ°ç›®å½•ï¼‰ï¼š

**åœ¨ CIFAR-10 æ•°æ®é›†ä¸Šå¾®è°ƒä¸€ä¸ªåœ¨ ImageNet-21k ä¸Šé¢„è®­ç»ƒè¿‡çš„ ViT-B/16 æ¨¡åž‹:ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ðŸ‘‡**
```bash
python -m vit_jax.main --workdir=/tmp/vit-$(date +%s) \
    --config=$(pwd)/vit_jax/configs/vit.py:b16,cifar10 \
    --config.pretrained_dir='gs://vit_models/imagenet21k'
```

python -m vit_jax.mainï¼šè¿è¡Œvit_jaxæ–‡ä»¶å¤¹ä¸‹çš„mainå‡½æ•°çš„pythonè„šæœ¬
--workdirï¼šç”Ÿæˆä¸€ä¸ªå·¥ä½œç›®å½•å¸¦æ—¶é—´æˆ³æ–‡ä»¶å¤¹(å¦‚/tmp/vit-1730793635/ï¼Œå…¶ä¸­1730793635å°±è¡¨ç¤ºè‡ª1970å¹´1æœˆ1æ—¥00:00:00 UTCï¼ˆUnix epochï¼‰èµ·åˆ°å½“å‰æ—¶åˆ»æ‰€ç»è¿‡çš„ç§’æ•°ã€‚ä»Žè€Œç¡®ä¿ç¡®æ–‡ä»¶å¤¹çš„å”¯ä¸€æ€§ä¸Žå¯è¿½æº¯æ€§)ï¼Œç”¨äºŽä¿å­˜è®­ç»ƒç»“æžœï¼ˆå¦‚æ—¥å¿—logsä¸Žæƒé‡checkpointsï¼‰ã€‚
--configï¼šæŒ‡å®šæ¨¡åž‹ä¸Žæ•°æ®é›†çš„é…ç½®æ–‡ä»¶è·¯å¾„ï¼Œ$(pwd) è¡¨ç¤ºå½“å‰å·¥ä½œç›®å½•è·¯å¾„ï¼Œæœ‰å½“å‰å·¥ä½œç›®å½•è·¯å¾„/vit_jax/configs/vit.pyæ–‡ä»¶,
          b16ï¼šä»£è¡¨ ViT-B/16 æ¨¡åž‹ç»“æž„,â€œBâ€ è¡¨ç¤º Base æ¨¡åž‹,â€œ16â€ è¡¨ç¤ºå›¾åƒè¢«åˆ’åˆ†ä¸º 16Ã—16 çš„ Patch å¤§å°ï¼›cifar10ï¼šè¡¨ç¤ºä½¿ç”¨ CIFAR-10 æ•°æ®é›† è¿›è¡Œè®­ç»ƒæˆ–å¾®è°ƒã€‚
--config.pretrained_dirï¼šå®šä¹‰é¢„è®­ç»ƒæ¨¡åž‹æƒé‡çš„è·¯å¾„ï¼Œè¿™é‡Œç›´æŽ¥ä»Ž Google Cloud Storage è¯»å–ï¼Œè€Œæ— éœ€æœ¬åœ°ä¸‹è½½ã€‚

**è¦åœ¨ CIFAR-10 æ•°æ®é›† ä¸Šå¾®è°ƒä¸€ä¸ªåœ¨ ImageNet-21k ä¸Šé¢„è®­ç»ƒè¿‡çš„ Mixer-B/16 æ¨¡åž‹:ä½¿ç”¨å¦‚ä¸‹å‘½ä»¤ðŸ‘‡**

```bash
python -m vit_jax.main --workdir=/tmp/vit-$(date +%s) \
    --config=$(pwd)/vit_jax/configs/mixer_base16_cifar10.py \
    --config.pretrained_dir='gs://mixer_models/imagenet21k'
```

è®ºæ–‡ã€ŠHow to train your ViT? ...ã€‹ä¸­æ–°å¢žäº†è¶…è¿‡ 5 ä¸‡ä¸ªæ¨¡åž‹æƒé‡ï¼ˆcheckpointsï¼‰çš„é¢„è®­ç»ƒæ¨¡åž‹ï¼Œ
ä½ å¯ä»¥ä½¿ç”¨ [`configs/augreg.py`] é…ç½®æ–‡ä»¶å¯¹è¿™äº›æ¨¡åž‹è¿›è¡Œå¾®è°ƒï¼ˆfine-tuningï¼‰ã€‚
å½“ä½ ä»…æŒ‡å®šæ¨¡åž‹åç§° ( å³ [`configs/model.py`] ä¸­çš„ `config.name`å‚æ•°å€¼)æ—¶, 
ç³»ç»Ÿä¼šè‡ªåŠ¨é€‰æ‹©åœ¨ä¸Šæ¸¸éªŒè¯é›†ä¸Šç²¾åº¦æœ€é«˜çš„ ImageNet-21k æœ€ä¼˜æƒé‡ï¼Œ ä¹Ÿå°±æ˜¯è®ºæ–‡ç¬¬ 4.5 èŠ‚ä¸­æåˆ°çš„â€œæŽ¨èï¼ˆrecommendedï¼‰â€æ¨¡åž‹ã€‚
å¦‚æžœä½ æƒ³äº†è§£å“ªç§æ¨¡åž‹æ›´é€‚åˆä½¿ç”¨ï¼Œå¯ä»¥å‚è€ƒè®ºæ–‡ä¸­çš„ å›¾ 3ï¼ˆFigure 3ï¼‰ã€‚
å½“ç„¶ï¼Œä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨æŒ‡å®šå…¶ä»–é¢„è®­ç»ƒæƒé‡æ–‡ä»¶ï¼Œ (å‚è€ƒ Colab ç¤ºä¾‹ [`vit_jax_augreg.ipynb`]) 
å…·ä½“æ–¹æ³•æ˜¯ï¼šåˆ°[`gs://vit_models/augreg`] ç›®å½•æŸ¥æ‰¾æƒ³è¦çš„æ¨¡åž‹æ–‡ä»¶åï¼ˆåŽ»æŽ‰ .npz åŽç¼€ï¼‰ï¼Œç„¶åŽåœ¨å‘½ä»¤ä¸­é€šè¿‡ --config.pretrained_dir å‚æ•°å‘Šè¯‰ç¨‹åºåŠ è½½å®ƒã€‚

**è¿è¡Œ ViT-JAX ä¸»è®­ç»ƒè„šæœ¬ï¼Œæ¨¡åž‹ç»“æž„æ˜¯ R_Ti_16ï¼Œæ•°æ®é›†æ˜¯Oxford-IIIT Pet ðŸ‘‡**
```bash
python -m vit_jax.main --workdir=/tmp/vit-$(date +%s) \
    --config=$(pwd)/vit_jax/configs/augreg.py:R_Ti_16 \
    --config.dataset=oxford_iiit_pet \
    --config.base_lr=0.01
```
å¦‚æžœè¿˜è¦åŠ æŒ‡ä»¤å¯ä»¥åŠ  

è‡ªå·±æŒ‡å®šçš„é¢„è®­ç»ƒæƒé‡ --config.pretrained_dir='gs://vit_models/augreg/B_16_i21k_ft1k'ï¼ˆæ¥è‡ª gs://vit_models/augreg/ ç›®å½•ï¼‰

æ‰¹é‡å¤§å°ï¼š--config.batch_size=256

è®­ç»ƒæ­¥æ•°ï¼š--config.total_steps=20000

æƒé‡è¡°å‡ï¼ˆL2 æ­£åˆ™åŒ–ï¼‰ï¼š--config.weight_decay=0.0001

è¾“å‡ºé—´éš”ï¼š--config.log_every_steps=100

å¦‚æžœè¦è®­ç»ƒç›´æŽ¥æ•°æ®é›†ï¼Œå¯ä»¥åŠ ä¸Šé¡¹ç›®ä¸¤è¡Œ

--config.dataset=my_dataset #ç”¨ ImageNet æ ¼å¼åŠ è½½æˆ‘çš„è‡ªå®šä¹‰æ•°æ®é›†

--config.dataset_dir=/home/ws/datasets/my_dataset #è‡ªå·±æ•°æ®é›†çš„ç›®å½•


ç›®å‰ï¼Œä»£ç ä¼šè‡ªåŠ¨ä¸‹è½½ CIFAR-10 å’Œ CIFAR-100 æ•°æ®é›†ã€‚
ä»–å…¬å…±æ•°æ®é›†æˆ–è‡ªå®šä¹‰æ•°æ®é›†ä¹Ÿå¯ä»¥å¾ˆå®¹æ˜“åœ°é›†æˆï¼Œåªéœ€ä½¿ç”¨ [tensorflow
datasets library](https://github.com/tensorflow/datasets/). 
è¯·æ³¨æ„ï¼Œå¦‚æžœä½ æ·»åŠ äº†æ–°çš„æ•°æ®é›†ï¼Œè¿˜éœ€è¦ä¿®æ”¹ `vit_jax/input_pipeline.py` æ–‡ä»¶ï¼Œä»¥æŒ‡å®šè¯¥æ•°æ®é›†çš„ä¸€äº›ç›¸å…³å‚æ•°ï¼ˆå¦‚å›¾åƒå¤§å°ã€é€šé“æ•°ã€ç±»åˆ«æ•°ç­‰ï¼‰ã€‚

ä»£ç åœ¨å¾®è°ƒï¼ˆfine-tuningï¼‰æ—¶ä¼šè‡ªåŠ¨ä½¿ç”¨æ‰€æœ‰å¯ç”¨çš„ GPU æˆ– TPUã€‚

è¦æŸ¥çœ‹æ‰€æœ‰å¯ç”¨çš„å‘½ä»¤è¡Œå‚æ•°ï¼ˆflagsï¼‰ï¼Œå¯ä»¥è¿è¡Œï¼š `python3 -m vit_jax.train
--help`.

å†…å­˜ä½¿ç”¨è¯´æ˜Žï¼š

- ä¸åŒæ¨¡åž‹å¯¹å†…å­˜çš„éœ€æ±‚ä¸åŒã€‚å®žé™…å¯ç”¨å†…å­˜è¿˜å–å†³äºŽåŠ é€Ÿå™¨ï¼ˆGPU/TPUï¼‰çš„ç±»åž‹å’Œæ•°é‡ã€‚å¦‚æžœé‡åˆ° æ˜¾å­˜ä¸è¶³ï¼ˆout-of-memory, OOMï¼‰ é”™è¯¯ï¼Œå¯ä»¥ï¼š
  å¢žå¤§ï¼ˆæ¢¯åº¦ç´¯ç§¯æ­¥æ•°ï¼‰`--config.accum_steps=8`ï¼Œä»¥é™ä½Žå•æ­¥æ˜¾å­˜å ç”¨æˆ–å‡å°`--config.batch=512` ï¼ˆæ‰¹é‡å¤§å°ï¼‰(åŒæ—¶ç›¸åº”åœ°é™ä½Ž `--config.base_lr` å­¦ä¹ çŽ‡).
- ä¸»æœºï¼ˆhostï¼‰åœ¨å†…å­˜ä¸­ä¼šç»´æŠ¤ä¸€ä¸ªæ•°æ®æ‰“ä¹±ç¼“å†²åŒºï¼ˆshuffle bufferï¼‰ã€‚
  å¦‚æžœå‡ºçŽ° ä¸»æœºå†…å­˜ä¸è¶³ï¼ˆhost OOMï¼‰ï¼Œè€Œä¸æ˜¯æ˜¾å¡æ˜¾å­˜ä¸è¶³ï¼Œå¯ä»¥é€‚å½“å‡å°é»˜è®¤çš„`--config.shuffle_buffer=50000`çš„å€¼


## Vision Transformer

by Alexey Dosovitskiy\*â€ , Lucas Beyer\*, Alexander Kolesnikov\*, Dirk
Weissenborn\*, Xiaohua Zhai\*, Thomas Unterthiner, Mostafa Dehghani, Matthias
Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit and Neil Houlsby\*â€ .

(\*) equal technical contribution, (â€ ) equal advising.

![Figure 1 from paper](vit_figure.png)

Overview of the model: we split an image into fixed-size patches, linearly embed
each of them, add position embeddings, and feed the resulting sequence of
vectors to a standard Transformer encoder. In order to perform classification,
we use the standard approach of adding an extra learnable "classification token"
to the sequence.

### Available ViT models

We provide a variety of ViT models in different GCS buckets. The models can be
downloaded with e.g.:

```
wget https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz
```

The model filenames (without the `.npz` extension) correspond to the
`config.model_name` in [`vit_jax/configs/models.py`]

- [`gs://vit_models/imagenet21k`] - Models pre-trained on ImageNet-21k.
- [`gs://vit_models/imagenet21k+imagenet2012`] - Models pre-trained on
  ImageNet-21k and fine-tuned on ImageNet.
- [`gs://vit_models/augreg`] - Models pre-trained on ImageNet-21k,
  applying varying amounts of [AugReg]. Improved performance.
- [`gs://vit_models/sam`] - Models pre-trained on ImageNet with [SAM].
- [`gs://vit_models/gsam`] - Models pre-trained on ImageNet with [GSAM].

We recommend using the following checkpoints, trained with [AugReg] that have
the best pre-training metrics:

|  Model   |                                   Pre-trained checkpoint                                   |   Size   |                                                       Fine-tuned checkpoint                                                        | Resolution | Img/sec | Imagenet accuracy |
| :------- | :----------------------------------------------------------------------------------------- | -------: | :--------------------------------------------------------------------------------------------------------------------------------- | ---------: | ------: | ----------------: |
| L/16     | `gs://vit_models/augreg/L_16-i21k-300ep-lr_0.001-aug_strong1-wd_0.1-do_0.0-sd_0.0.npz`     | 1243 MiB | `gs://vit_models/augreg/L_16-i21k-300ep-lr_0.001-aug_strong1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz`     |        384 |      50 |            85.59% |
| B/16     | `gs://vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz`     |  391 MiB | `gs://vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz`     |        384 |     138 |            85.49% |
| S/16     | `gs://vit_models/augreg/S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0.npz`     |  115 MiB | `gs://vit_models/augreg/S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz`     |        384 |     300 |            83.73% |
| R50+L/32 | `gs://vit_models/augreg/R50_L_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1.npz` | 1337 MiB | `gs://vit_models/augreg/R50_L_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_384.npz` |        384 |     327 |            85.99% |
| R26+S/32 | `gs://vit_models/augreg/R26_S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0.npz`  |  170 MiB | `gs://vit_models/augreg/R26_S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz`  |        384 |     560 |            83.85% |
| Ti/16    | `gs://vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0.npz`      |   37 MiB | `gs://vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz`      |        384 |     610 |            78.22% |
| B/32     | `gs://vit_models/augreg/B_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0.npz`      |  398 MiB | `gs://vit_models/augreg/B_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz`      |        384 |     955 |            83.59% |
| S/32     | `gs://vit_models/augreg/S_32-i21k-300ep-lr_0.001-aug_none-wd_0.1-do_0.0-sd_0.0.npz`        |  118 MiB | `gs://vit_models/augreg/S_32-i21k-300ep-lr_0.001-aug_none-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz`        |        384 |    2154 |            79.58% |
| R+Ti/16  | `gs://vit_models/augreg/R_Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0.npz`    |   40 MiB | `gs://vit_models/augreg/R_Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz`    |        384 |    2426 |            75.40% |

The results from the original ViT paper (https://arxiv.org/abs/2010.11929) have
been replicated using the models from [`gs://vit_models/imagenet21k`]:

| model        | dataset      | dropout=0.0                                                                                                                                                         | dropout=0.1                                                                                                                                                          |
|:-------------|:-------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| R50+ViT-B_16 | cifar10      | 98.72%, 3.9h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5ER50.ViT-B_16/cifar10/do_0.0&_smoothingWeight=0)      | 98.94%, 10.1h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5ER50.ViT-B_16/cifar10/do_0.1&_smoothingWeight=0)      |
| R50+ViT-B_16 | cifar100     | 90.88%, 4.1h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5ER50.ViT-B_16/cifar100/do_0.0&_smoothingWeight=0)     | 92.30%, 10.1h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5ER50.ViT-B_16/cifar100/do_0.1&_smoothingWeight=0)     |
| R50+ViT-B_16 | imagenet2012 | 83.72%, 9.9h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5ER50.ViT-B_16/imagenet2012/do_0.0&_smoothingWeight=0) | 85.08%, 24.2h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5ER50.ViT-B_16/imagenet2012/do_0.1&_smoothingWeight=0) |
| ViT-B_16     | cifar10      | 99.02%, 2.2h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_16/cifar10/do_0.0&_smoothingWeight=0)          | 98.76%, 7.8h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_16/cifar10/do_0.1&_smoothingWeight=0)           |
| ViT-B_16     | cifar100     | 92.06%, 2.2h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_16/cifar100/do_0.0&_smoothingWeight=0)         | 91.92%, 7.8h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_16/cifar100/do_0.1&_smoothingWeight=0)          |
| ViT-B_16     | imagenet2012 | 84.53%, 6.5h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_16/imagenet2012/do_0.0&_smoothingWeight=0)     | 84.12%, 19.3h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_16/imagenet2012/do_0.1&_smoothingWeight=0)     |
| ViT-B_32     | cifar10      | 98.88%, 0.8h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_32/cifar10/do_0.0&_smoothingWeight=0)          | 98.75%, 1.8h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_32/cifar10/do_0.1&_smoothingWeight=0)           |
| ViT-B_32     | cifar100     | 92.31%, 0.8h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_32/cifar100/do_0.0&_smoothingWeight=0)         | 92.05%, 1.8h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_32/cifar100/do_0.1&_smoothingWeight=0)          |
| ViT-B_32     | imagenet2012 | 81.66%, 3.3h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_32/imagenet2012/do_0.0&_smoothingWeight=0)     | 81.31%, 4.9h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_32/imagenet2012/do_0.1&_smoothingWeight=0)      |
| ViT-L_16     | cifar10      | 99.13%, 6.9h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_16/cifar10/do_0.0&_smoothingWeight=0)          | 99.14%, 24.7h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_16/cifar10/do_0.1&_smoothingWeight=0)          |
| ViT-L_16     | cifar100     | 92.91%, 7.1h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_16/cifar100/do_0.0&_smoothingWeight=0)         | 93.22%, 24.4h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_16/cifar100/do_0.1&_smoothingWeight=0)         |
| ViT-L_16     | imagenet2012 | 84.47%, 16.8h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_16/imagenet2012/do_0.0&_smoothingWeight=0)    | 85.05%, 59.7h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_16/imagenet2012/do_0.1&_smoothingWeight=0)     |
| ViT-L_32     | cifar10      | 99.06%, 1.9h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_32/cifar10/do_0.0&_smoothingWeight=0)          | 99.09%, 6.1h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_32/cifar10/do_0.1&_smoothingWeight=0)           |
| ViT-L_32     | cifar100     | 93.29%, 1.9h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_32/cifar100/do_0.0&_smoothingWeight=0)         | 93.34%, 6.2h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_32/cifar100/do_0.1&_smoothingWeight=0)          |
| ViT-L_32     | imagenet2012 | 81.89%, 7.5h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_32/imagenet2012/do_0.0&_smoothingWeight=0)     | 81.13%, 15.0h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_32/imagenet2012/do_0.1&_smoothingWeight=0)     |

We also would like to emphasize that high-quality results can be achieved with
shorter training schedules and encourage users of our code to play with
hyper-parameters to trade-off accuracy and computational budget.
Some examples for CIFAR-10/100 datasets are presented in the table below.

| upstream    | model    | dataset      | total_steps / warmup_steps  | accuracy | wall-clock time |                                                                         link |
| ----------- | -------- | ------------ | --------------------------- | -------- | --------------- | ---------------------------------------------------------------------------- |
| imagenet21k | ViT-B_16 | cifar10      | 500 / 50                    |   98.59% |             17m | [tensorboard.dev](https://tensorboard.dev/experiment/QgkpiW53RPmjkabe1ME31g/) |
| imagenet21k | ViT-B_16 | cifar10      | 1000 / 100                  |   98.86% |             39m | [tensorboard.dev](https://tensorboard.dev/experiment/w8DQkDeJTOqJW5js80gOQg/) |
| imagenet21k | ViT-B_16 | cifar100     | 500 / 50                    |   89.17% |             17m | [tensorboard.dev](https://tensorboard.dev/experiment/5hM4GrnAR0KEZg725Ewnqg/) |
| imagenet21k | ViT-B_16 | cifar100     | 1000 / 100                  |   91.15% |             39m | [tensorboard.dev](https://tensorboard.dev/experiment/QLQTaaIoT9uEcAjtA0eRwg/) |


## MLP-Mixer

by Ilya Tolstikhin\*, Neil Houlsby\*, Alexander Kolesnikov\*, Lucas Beyer\*,
Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers,
Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy.

(\*) equal contribution.

![Figure 1 from paper](mixer_figure.png)

MLP-Mixer (*Mixer* for short) consists of per-patch linear embeddings, Mixer
layers, and a classifier head. Mixer layers contain one token-mixing MLP and one
channel-mixing MLP, each consisting of two fully-connected layers and a GELU
nonlinearity. Other components include: skip-connections, dropout, and linear
classifier head.

For installation follow [the same steps](#installation) as above.

### Available Mixer models

We provide the Mixer-B/16 and Mixer-L/16 models pre-trained on the ImageNet and
ImageNet-21k datasets. Details can be found in Table 3 of the Mixer paper. All
the models can be found at:

https://console.cloud.google.com/storage/mixer_models/

Note that these models are also available directly from TF-Hub:
[sayakpaul/collections/mlp-mixer] (external contribution by [Sayak
Paul]).

[sayakpaul/collections/mlp-mixer]: https://tfhub.dev/sayakpaul/collections/mlp-mixer

### Expected Mixer results

We ran the fine-tuning code on Google Cloud machine with four V100 GPUs with the
default adaption parameters from this repository. Here are the results:

upstream     | model      | dataset | accuracy | wall_clock_time | link
:----------- | :--------- | :------ | -------: | :-------------- | :---
ImageNet     | Mixer-B/16 | cifar10 | 96.72%   | 3.0h            | [tensorboard.dev](https://tensorboard.dev/experiment/j9zCYt9yQVm93nqnsDZayA/)
ImageNet     | Mixer-L/16 | cifar10 | 96.59%   | 3.0h            | [tensorboard.dev](https://tensorboard.dev/experiment/Q4feeErzRGGop5XzAvYj2g/)
ImageNet-21k | Mixer-B/16 | cifar10 | 96.82%   | 9.6h            | [tensorboard.dev](https://tensorboard.dev/experiment/mvP4McV2SEGFeIww20ie5Q/)
ImageNet-21k | Mixer-L/16 | cifar10 | 98.34%   | 10.0h           | [tensorboard.dev](https://tensorboard.dev/experiment/dolAJyQYTYmudytjalF6Jg/)


## LiT models

For details, refer to the Google AI blog post
[LiT: adding language understanding to image models](http://ai.googleblog.com/2022/04/locked-image-tuning-adding-language.html),
or read the CVPR paper "LiT: Zero-Shot Transfer with Locked-image text Tuning"
(https://arxiv.org/abs/2111.07991).

We published a Transformer B/16-base model with an ImageNet zeroshot accuracy of
72.1%, and a L/16-large model with an ImageNet zeroshot accuracy of 75.7%. For
more details about these models, please refer to the
[LiT model card](model_cards/lit.md).

We provide a in-browser demo with small text encoders for interactive use (the
smallest models should even run on a modern cell phone):

https://google-research.github.io/vision_transformer/lit/

And finally a Colab to use the JAX models with both image and text encoders:

https://colab.research.google.com/github/google-research/vision_transformer/blob/main/lit.ipynb

Note that none of above models support multi-lingual inputs yet, but we're
working on publishing such models and will update this repository once they
become available.

This repository only contains evaluation code for LiT models. You can find the
training code in the `big_vision` repository:

https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text

Expected zeroshot results from [`model_cards/lit.md`] (note that the zeroshot
evaluation is slightly different from the simplified evaluation in the Colab):

| Model | B16B_2 | L16L |
| :--- | ---: | ---: |
| ImageNet zero-shot | 73.9% | 75.7% |
| ImageNet v2 zero-shot | 65.1% | 66.6% |
| CIFAR100 zero-shot | 79.0% | 80.5% |
| Pets37 zero-shot | 83.3% | 83.3% |
| Resisc45 zero-shot | 25.3% | 25.6% |
| MS-COCO Captions image-to-text retrieval | 51.6% | 48.5% |
| MS-COCO Captions text-to-image retrieval | 31.8% | 31.1% |

## Running on cloud

While above [colabs](#colab) are pretty useful to get started, you would usually
want to train on a larger machine with more powerful accelerators.

### Create a VM

You can use the following commands to setup a VM with GPUs on Google Cloud:

```bash
# Set variables used by all commands below.
# Note that project must have accounting set up.
# For a list of zones with GPUs refer to
# https://cloud.google.com/compute/docs/gpus/gpu-regions-zones
PROJECT=my-awesome-gcp-project  # Project must have billing enabled.
VM_NAME=vit-jax-vm-gpu
ZONE=europe-west4-b

# Below settings have been tested with this repository. You can choose other
# combinations of images & machines (e.g.), refer to the corresponding gcloud commands:
# gcloud compute images list --project ml-images
# gcloud compute machine-types list
# etc.
gcloud compute instances create $VM_NAME \
    --project=$PROJECT --zone=$ZONE \
    --image=c1-deeplearning-tf-2-5-cu110-v20210527-debian-10 \
    --image-project=ml-images --machine-type=n1-standard-96 \
    --scopes=cloud-platform,storage-full --boot-disk-size=256GB \
    --boot-disk-type=pd-ssd --metadata=install-nvidia-driver=True \
    --maintenance-policy=TERMINATE \
    --accelerator=type=nvidia-tesla-v100,count=8

# Connect to VM (after some minutes needed to setup & start the machine).
gcloud compute ssh --project $PROJECT --zone $ZONE $VM_NAME

# Stop the VM after use (only storage is billed for a stopped VM).
gcloud compute instances stop --project $PROJECT --zone $ZONE $VM_NAME

# Delete VM after use (this will also remove all data stored on VM).
gcloud compute instances delete --project $PROJECT --zone $ZONE $VM_NAME
```

Alternatively, you can use the following similar commands to set up a Cloud VM
with TPUs attached to them (below commands copied from the [TPU tutorial]):

[TPU tutorial]: https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm

```bash
PROJECT=my-awesome-gcp-project  # Project must have billing enabled.
VM_NAME=vit-jax-vm-tpu
ZONE=europe-west4-a

# Required to set up service identity initially.
gcloud beta services identity create --service tpu.googleapis.com

# Create a VM with TPUs directly attached to it.
gcloud alpha compute tpus tpu-vm create $VM_NAME \
    --project=$PROJECT --zone=$ZONE \
    --accelerator-type v3-8 \
    --version tpu-vm-base

# Connect to VM (after some minutes needed to setup & start the machine).
gcloud alpha compute tpus tpu-vm ssh --project $PROJECT --zone $ZONE $VM_NAME

# Stop the VM after use (only storage is billed for a stopped VM).
gcloud alpha compute tpus tpu-vm stop --project $PROJECT --zone $ZONE $VM_NAME

# Delete VM after use (this will also remove all data stored on VM).
gcloud alpha compute tpus tpu-vm delete --project $PROJECT --zone $ZONE $VM_NAME
```

### Setup VM

And then fetch the repository and the install dependencies (including `jaxlib`
with TPU support) as usual:

```bash
git clone --depth=1 --branch=master https://github.com/google-research/vision_transformer
cd vision_transformer

# optional: install virtualenv
pip3 install virtualenv
python3 -m virtualenv env
. env/bin/activate
```

If you're connected to a VM with GPUs attached, install JAX and other dependencies with the following
command:

```bash
pip install -r vit_jax/requirements.txt
```

If you're connected to a VM with TPUs attached, install JAX and other dependencies with the following
command:

```bash
pip install -r vit_jax/requirements-tpu.txt
```

Install [Flaxformer](https://github.com/google/flaxformer), follow the instructions
provided in the corresponding repository linked here.

For both GPUs and TPUs, Check that JAX can connect to attached accelerators with the command:
```bash
python -c 'import jax; print(jax.devices())'
```

And finally execute one of the commands mentioned in the section
[fine-tuning a model](#fine-tuning-a-model).


## Bibtex

```
@article{dosovitskiy2020vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
}

@article{tolstikhin2021mixer,
  title={MLP-Mixer: An all-MLP Architecture for Vision},
  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2105.01601},
  year={2021}
}

@article{steiner2021augreg,
  title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},
  author={Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  journal={arXiv preprint arXiv:2106.10270},
  year={2021}
}

@article{chen2021outperform,
  title={When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations},
  author={Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing},
  journal={arXiv preprint arXiv:2106.01548},
  year={2021},
}

@article{zhuang2022gsam,
  title={Surrogate Gap Minimization Improves Sharpness-Aware Training},
  author={Zhuang, Juntang and Gong, Boqing and Yuan, Liangzhe and Cui, Yin and Adam, Hartwig and Dvornek, Nicha and Tatikonda, Sekhar and Duncan, James and Liu, Ting},
  journal={ICLR},
  year={2022},
}

@article{zhai2022lit,
  title={LiT: Zero-Shot Transfer with Locked-image Text Tuning},
  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
  journal={CVPR},
  year={2022}
}
```


## Changelog

In reverse chronological order:

- 2022-08-18: Added LiT-B16B_2 model that was trained for 60k steps
  (LiT_B16B: 30k) without linear head on the image side (LiT_B16B: 768) and has
  better performance.

- 2022-06-09: Added the ViT and Mixer models trained from scratch using
  [GSAM] on ImageNet without strong data augmentations. The resultant ViTs
  outperform those of similar sizes trained using AdamW optimizer or the
  original [SAM] algorithm, or with strong data augmentations.

- 2022-04-14: Added models and Colab for [LiT models](#lit-models).

- 2021-07-29: Added ViT-B/8 AugReg models (3 upstream checkpoints and adaptations
  with resolution=224).

- 2021-07-02: Added the "When Vision Transformers Outperform
  ResNets..." paper

- 2021-07-02: Added [SAM](https://arxiv.org/abs/2010.01412)
  (Sharpness-Aware Minimization) optimized ViT and MLP-Mixer checkpoints.

- 2021-06-20: Added the "How to train your ViT? ..." paper, and a new
  Colab to explore the >50k pre-trained and fine-tuned checkpoints mentioned in
  the paper.

- 2021-06-18: This repository was rewritten to use Flax Linen API and
  `ml_collections.ConfigDict` for configuration.

- 2021-05-19: With publication of the "How to train your ViT? ..."
  paper, we added more than 50k ViT and hybrid models pre-trained on ImageNet and
  ImageNet-21k with various degrees of data augmentation and model regularization,
  and fine-tuned on ImageNet, Pets37, Kitti-distance, CIFAR-100, and Resisc45.
  Check out [`vit_jax_augreg.ipynb`] to navigate this treasure trove of models!
  For example, you can use that Colab to fetch the filenames of recommended
  pre-trained and fine-tuned checkpoints from the `i21k_300` column of Table 3 in
  the paper.

- 2020-12-01: Added the R50+ViT-B/16 hybrid model (ViT-B/16 on
  top of a Resnet-50 backbone). When pretrained on imagenet21k, this model
  achieves almost the performance of the L/16 model with less than half the
  computational finetuning cost. Note that "R50" is somewhat modified for the
  B/16 variant: The original ResNet-50 has [3,4,6,3] blocks, each reducing the
  resolution of the image by a factor of two. In combination with the ResNet
  stem this would result in a reduction of 32x so even with a patch size of
  (1,1) the ViT-B/16 variant cannot be realized anymore. For this reason we
  instead use [3,4,9] blocks for the R50+B/16 variant.

- 2020-11-09: Added the ViT-L/16 model.

- 2020-10-29: Added ViT-B/16 and ViT-L/16 models pretrained
  on ImageNet-21k and then fine-tuned on ImageNet at 224x224 resolution (instead
  of default 384x384). These models have the suffix "-224" in their name.
  They are expected to achieve 81.2% and 82.7% top-1 accuracies respectively.


## Disclaimers

Open source release prepared by Andreas Steiner.

Note: This repository was forked and modified from
[google-research/big_transfer](https://github.com/google-research/big_transfer).

**This is not an official Google product.**


[GSAM]: https://arxiv.org/abs/2203.08065
[SAM]: https://arxiv.org/abs/2010.01412
[AugReg]: https://arxiv.org/abs/2106.10270

[`vit_jax/configs/models.py`]: https://github.com/google-research/vision_transformer/blob/main/vit_jax/configs/models.py
[`model_cards/lit.md`]: https://github.com/google-research/vision_transformer/blob/main/model_cards/lit.md

[`configs/augreg.py`]: https://github.com/google-research/vision_transformer/blob/main/vit_jax/configs/augreg.py
[`configs/model.py`]: https://github.com/google-research/vision_transformer/blob/main/vit_jax/configs/models.py
[`vit_jax_augreg.ipynb`]: https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax_augreg.ipynb
[`vit_jax.ipynb`]: https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax.ipynb

[`gs://vit_models/imagenet21k`]: https://console.cloud.google.com/storage/browser/vit_models/imagenet21k/
[`gs://vit_models/imagenet21k+imagenet2012`]: https://console.cloud.google.com/storage/browser/vit_models/imagenet21k+imagenet2012/
[`gs://vit_models/augreg`]: https://console.cloud.google.com/storage/browser/vit_models/augreg/
[`gs://vit_models/sam`]: https://console.cloud.google.com/storage/browser/vit_models/sam/
[`gs://mixer_models/sam`]: https://console.cloud.google.com/storage/mixer_models/sam/
[`gs://vit_models/gsam`]: https://console.cloud.google.com/storage/browser/vit_models/gsam/
[`gs://mixer_models/gsam`]: https://console.cloud.google.com/storage/mixer_models/gsam/
