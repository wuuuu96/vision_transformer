# Vision Transformer Âíå MLP-Mixer Êû∂ÊûÑ

Âú®Êú¨‰ªìÂ∫ì‰∏≠ÔºåÊàë‰ª¨ÂèëÂ∏É‰∫ÜËøô‰∫õËÆ∫Êñá‰∏≠ÊâÄ‰ΩøÁî®ÁöÑÊ®°Âûã„ÄÇ

- (ViT) [An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale](https://arxiv.org/abs/2010.11929)
- (MLP-Mixer) [MLP-Mixer: An all-MLP Architecture for Vision](https://arxiv.org/abs/2105.01601)
- [How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers](https://arxiv.org/abs/2106.10270)
- [When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations](https://arxiv.org/abs/2106.01548)
- (LiT) [LiT: Zero-Shot Transfer with Locked-image text Tuning](https://arxiv.org/abs/2111.07991)
- [Surrogate Gap Minimization Improves Sharpness-Aware Training](https://arxiv.org/abs/2203.08065)


Ëøô‰∫õÊ®°ÂûãÂú® [ImageNet](http://www.image-net.org/) Âíå [ImageNet-21k](http://www.image-net.org/) Êï∞ÊçÆÈõÜ‰∏äËøõË°å‰∫ÜÈ¢ÑËÆ≠ÁªÉ„ÄÇ

‰ΩøÁî® [JAX](https://jax.readthedocs.io) Âíå [Flax](http://flax.readthedocs.io) Ê°ÜÊû∂ÁºñÂÜôÁöÑÊ∫ê‰ª£Á†ÅÔºåÂπ∂‰∏îÂú®Â∑≤ÊúâÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÁöÑÂü∫Á°Ä‰∏äÁªßÁª≠ÂæÆË∞ÉËÆ≠ÁªÉÔºå‰ª•ÈÄÇÈÖçÊñ∞ÁöÑ‰ªªÂä°ÊàñÊï∞ÊçÆÈõÜ„ÄÇ



Ëøô‰∫õÊ®°ÂûãÊúÄÂàùÊòØÂú®‰ª•‰∏ã‰ª£Á†ÅÂ∫ì‰∏≠ËÆ≠ÁªÉÁöÑÔºöüëâ https://github.com/google-research/big_vision/

Âú®ÈÇ£ÈáåÔºå‰Ω†ÂèØ‰ª•ÊâæÂà∞Êõ¥È´òÁ∫ßÁöÑ‰ª£Á†ÅÔºà‰æãÂ¶Ç Â§ö‰∏ªÊú∫ËÆ≠ÁªÉÔºàmulti-host trainingÔºâÔºâÔºå‰ª•Âèä‰∏Ä‰∫õÊúÄÂàùÁöÑËÆ≠ÁªÉËÑöÊú¨Ôºå‰æãÂ¶ÇÔºö

configs/vit_i21k.py
ÔºöÁî®‰∫é È¢ÑËÆ≠ÁªÉ ViTÔºàVision TransformerÔºâÊ®°ÂûãÔºõ

configs/transfer.py
ÔºöÁî®‰∫é ËøÅÁßªÂ∑≤ÊúâÊ®°ÂûãÔºàtransfer learningÔºâ„ÄÇ

ÁõÆÂΩï:

- [ËßÜËßâTransformerÂíåMLP-MixerÊû∂ÊûÑ](#vision-transformer-and-mlp-mixer-architectures)
	- [ColabÂú®Á∫øËøêË°å](#Colab)
	- [ÂÆâË£ÖÊ≠•È™§](#installation)
	- [ÂæÆË∞ÉÊ®°Âûã](#fine-tuning-a-model)
	- [ËßÜËßâTransformerÔºàViTÔºâÊ®°Âûã](#vision-transformer)
		- [ÂèØÁî®ÁöÑViTÊ®°Âûã](#available-vit-models)
		- [ViTÁöÑÈ¢ÑÊúüÁªìÊûú](#expected-vit-results)
	- [MLP-MixerÊ®°Âûã](#mlp-mixer)
		- [ÂèØÁî®ÁöÑMixerÊ®°Âûã](#available-mixer-models)
		- [MixerÁöÑÈ¢ÑÊúüÁªìÊûú](#expected-mixer-results)
	- [LiTÊ®°Âûã](#lit-models)
	- [‰∫ëÁ´ØËøêË°å](#running-on-cloud)
		- [ÂàõÂª∫ËôöÊãüÊú∫](#create-a-vm)
		- [ÈÖçÁΩÆËôöÊãüÊú∫](#setup-vm)
	- [ÂèÇËÄÉÊñáÁåÆBibTeXÊù°ÁõÆ](#bibtex)
	- [ÂÖçË¥£Â£∞Êòé](#disclaimers)
	- [Êõ¥Êñ∞Êó•Âøó](#changelog)


## Colab

‰ª•‰∏ã‰∏§‰∏™ Colab Á§∫‰æãÈÉΩÂèØ‰ª•Âú® GPU Êàñ TPUÔºà8 Ê†∏Êï∞ÊçÆÂπ∂Ë°åÔºâ ÁéØÂ¢É‰∏ãËøêË°å„ÄÇ

üîπÁ¨¨‰∏Ä‰∏™ Colab

ÊºîÁ§∫‰∫Ü Vision Transformer (ViT) Âíå MLP-Mixer ÁöÑ JAX ÂÆûÁé∞‰ª£Á†Å„ÄÇ

Âú®Ëøô‰∏™ Colab ‰∏≠Ôºå‰Ω†ÂèØ‰ª•Ôºö

Áõ¥Êé•Âú® Colab ÁïåÈù¢‰∏≠ÁºñËæë‰ªìÂ∫ì‰∏≠ÁöÑÊñá‰ª∂Ôºõ

ÈÄöËøáÂ∏¶Ê≥®ÈáäÁöÑ‰ª£Á†ÅÂçïÂÖÉÊ†ºÔºàannotated cellsÔºâÈÄêÊ≠•Â≠¶‰π†‰ª£Á†ÅÈÄªËæëÔºõ

‰∫§‰∫íÂºèÂú∞Êìç‰Ωú‰∏éÂèØËßÜÂåñÊï∞ÊçÆ„ÄÇ

https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax.ipynb


üîπÁ¨¨‰∫å‰∏™ Colab

ËØ•Á§∫‰æãÁî®‰∫éÊé¢Á¥¢Ë∂ÖËøá 5 ‰∏á‰∏™ Vision Transformer ‰∏éÊ∑∑ÂêàÊ®°ÂûãÔºàhybridÔºâÊ£ÄÊü•ÁÇπÔºàcheckpointsÔºâÔºåËøô‰∫õÊ®°ÂûãÊòØËÆ∫Êñá
„ÄäHow to train your ViT? ...„Äã
‰∏≠ÁîüÊàêÂÆûÈ™åÊï∞ÊçÆÊâÄÁî®ÁöÑÊ®°Âûã„ÄÇ

ËØ• Colab ÂåÖÂê´‰ª•‰∏ãÂäüËÉΩÔºö

Êèê‰æõ Ê£ÄÊü•ÁÇπÊµèËßà‰∏éÈÄâÊã© ÁöÑ‰ª£Á†ÅÔºõ

ÊîØÊåÅ‰ΩøÁî®Êú¨‰ªìÂ∫ì‰∏≠ÁöÑ JAX ‰ª£Á†Å Êàñ PyTorch ÁöÑ [`timm`] Â∫ìËøõË°åÊé®ÁêÜÔºàtimm ÂèØÁõ¥Êé•Âä†ËΩΩËøô‰∫õÊ®°ÂûãÔºâÔºõ

ÈÉ®ÂàÜÊ®°Âûã‰πüÂ∑≤Áõ¥Êé•ÂèëÂ∏ÉÂú® TensorFlow Hub ‰∏äÔºàÁî± [Sayak Paul] Êèê‰æõÁöÑÂ§ñÈÉ®Ë¥°ÁåÆÔºâÔºå‰æãÂ¶Ç
[sayakpaul/collections/vision_transformer]


Ê≠§Â§ñÔºåËØ• Colab ËøòÊîØÊåÅÔºö

ÂØπËøô‰∫õÈ¢ÑËÆ≠ÁªÉÊ£ÄÊü•ÁÇπËøõË°åÂæÆË∞ÉÔºàfine-tuningÔºâÔºõ

ÊîØÊåÅ‰ªªÊÑè tfds Êï∞ÊçÆÈõÜ Êàñ ‰Ω†Ëá™Â∑±ÁöÑ JPEG ÂõæÂÉèÊï∞ÊçÆÈõÜÔºàÂèØÁõ¥Êé•‰ªé Google Drive ËØªÂèñÔºâ„ÄÇ

https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax_augreg.ipynb



‚ö†Ô∏è Ê≥®ÊÑè‰∫ãÈ°πÔºàÊà™Ëá≥ 2021 Âπ¥ 6 Êúà 20 Êó•Ôºâ

Google Colab ÂΩìÂâç‰ªÖÊîØÊåÅÂçï‰∏™ GPUÔºàNVIDIA Tesla T4ÔºâÔºõ

TPUÔºàTPUv2-8Ôºâ ‰∏é Colab ËôöÊãüÊú∫ÊòØÈÄöËøáÁΩëÁªúÈó¥Êé•ËøûÊé•ÁöÑÔºåÈÄö‰ø°Âª∂ËøüËæÉÈ´òÔºåÂØºËá¥ËÆ≠ÁªÉÈÄüÂ∫¶ËæÉÊÖ¢Ôºõ

Ëã•‰Ω†ÁöÑÂæÆË∞É‰ªªÂä°Ê∂âÂèäÂ§ßÈáèÊï∞ÊçÆÔºåÂª∫ËÆÆÊê≠Âª∫Áã¨Á´ãÊúçÂä°Âô®Êàñ‰∫ëÁ´ØÂÆû‰æãÔºõ

ÂÖ∑‰ΩìÈÉ®ÁΩ≤ÊñπÂºèËØ¶ËßÅÁ´†ËäÇ[Running on cloud](#running-on-cloud)

[`timm`]: https://github.com/rwightman/pytorch-image-models
[sayakpaul/collections/vision_transformer]: https://tfhub.dev/sayakpaul/collections/vision_transformer
[Sayak Paul]: https://github.com/sayakpaul



## Installation

`Python>=3.10` 

Install JAX and python dependencies by running:

```
# If using GPU:
pip install -r vit_jax/requirements.txt

# If using TPU:
pip install -r vit_jax/requirements-tpu.txt
```

ÂØπ‰∫éÊñ∞ÁâàÁöÑ [JAX](https://github.com/google/jax), ËØ∑ÊåâÁÖßËØ•‰ªìÂ∫ì‰∏≠Êèê‰æõÁöÑÂÆâË£ÖËØ¥ÊòéËøõË°åÊìç‰Ωú„ÄÇ

ÈúÄË¶ÅÊ≥®ÊÑèÁöÑÊòØÔºåCPU„ÄÅGPU Âíå TPU ÁöÑÂÆâË£ÖÊ≠•È™§Áï•Êúâ‰∏çÂêå„ÄÇ

ÂÆâË£Ö [Flaxformer](https://github.com/google/flaxformer), ÂêåÊ†∑ËØ∑ÈÅµÂæ™ÂÖ∂ÂØπÂ∫î‰ªìÂ∫ì‰∏≠ÁöÑÂÆâË£ÖËØ¥Êòé„ÄÇ

Â¶ÇÈúÄ‰∫ÜËß£Êõ¥Â§öËØ¶ÊÉÖÔºåËØ∑ÂèÇËÄÉ‰∏ãÊñáÁöÑ‰∫ëÁ´ØËøêË°åÈÉ®ÂàÜ [Running on cloud](#running-on-cloud)



## Fine-tuning a model

‰Ω†ÂèØ‰ª•Âú®Ëá™Â∑±ÊÑüÂÖ¥Ë∂£ÁöÑÊï∞ÊçÆÈõÜ‰∏äÂØπ‰∏ãËΩΩÁöÑÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºàfine-tuningÔºâ„ÄÇÊâÄÊúâÊ®°ÂûãÈÉΩ‰ΩøÁî®Áõ∏ÂêåÁöÑÂëΩ‰ª§Ë°åÊé•Âè£„ÄÇ

‰æãÂ¶ÇÔºåË¶Å**Âú® CIFAR-10 Êï∞ÊçÆÈõÜ‰∏äÂæÆË∞É‰∏Ä‰∏™Âú® ImageNet-21k ‰∏äÈ¢ÑËÆ≠ÁªÉËøáÁöÑ ViT-B/16 Ê®°Âûã**
ÔºàËØ∑Ê≥®ÊÑèÔºåÊàë‰ª¨Âú®ÈÖçÁΩÆÂèÇÊï∞‰∏≠‰ΩøÁî®‰∫Ü b16,cifar10ÔºåÂπ∂ÈÄöËøá --config.pretrained_dir ËÆ©‰ª£Á†ÅÁõ¥Êé•‰ªé GCS ‰∫ëÁ´ØÂ≠òÂÇ®Ê°∂ ËØªÂèñÊ®°ÂûãÔºåËÄå‰∏çÊòØÂÖà‰∏ãËΩΩÂà∞Êú¨Âú∞ÁõÆÂΩïÔºâÔºö

**Âú® CIFAR-10 Êï∞ÊçÆÈõÜ‰∏äÂæÆË∞É‰∏Ä‰∏™Âú® ImageNet-21k ‰∏äÈ¢ÑËÆ≠ÁªÉËøáÁöÑ ViT-B/16 Ê®°Âûã:‰ΩøÁî®Â¶Ç‰∏ãÂëΩ‰ª§üëá**
```bash
python -m vit_jax.main --workdir=/tmp/vit-$(date +%s) \
    --config=$(pwd)/vit_jax/configs/vit.py:b16,cifar10 \
    --config.pretrained_dir='gs://vit_models/imagenet21k'
```

python -m vit_jax.mainÔºöËøêË°åvit_jaxÊñá‰ª∂Â§π‰∏ãÁöÑmainÂáΩÊï∞ÁöÑpythonËÑöÊú¨
--workdirÔºöÁîüÊàê‰∏Ä‰∏™Â∑•‰ΩúÁõÆÂΩïÂ∏¶Êó∂Èó¥Êà≥Êñá‰ª∂Â§π(Â¶Ç/tmp/vit-1730793635/ÔºåÂÖ∂‰∏≠1730793635Â∞±Ë°®Á§∫Ëá™1970Âπ¥1Êúà1Êó•00:00:00 UTCÔºàUnix epochÔºâËµ∑Âà∞ÂΩìÂâçÊó∂ÂàªÊâÄÁªèËøáÁöÑÁßíÊï∞„ÄÇ‰ªéËÄåÁ°Æ‰øùÁ°ÆÊñá‰ª∂Â§πÁöÑÂîØ‰∏ÄÊÄß‰∏éÂèØËøΩÊ∫ØÊÄß)ÔºåÁî®‰∫é‰øùÂ≠òËÆ≠ÁªÉÁªìÊûúÔºàÂ¶ÇÊó•Âøólogs‰∏éÊùÉÈáçcheckpointsÔºâ„ÄÇ
--configÔºöÊåáÂÆöÊ®°Âûã‰∏éÊï∞ÊçÆÈõÜÁöÑÈÖçÁΩÆÊñá‰ª∂Ë∑ØÂæÑÔºå$(pwd) Ë°®Á§∫ÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩïË∑ØÂæÑÔºåÊúâÂΩìÂâçÂ∑•‰ΩúÁõÆÂΩïË∑ØÂæÑ/vit_jax/configs/vit.pyÊñá‰ª∂,
          b16Ôºö‰ª£Ë°® ViT-B/16 Ê®°ÂûãÁªìÊûÑ,‚ÄúB‚Äù Ë°®Á§∫ Base Ê®°Âûã,‚Äú16‚Äù Ë°®Á§∫ÂõæÂÉèË¢´ÂàíÂàÜ‰∏∫ 16√ó16 ÁöÑ Patch Â§ßÂ∞èÔºõcifar10ÔºöË°®Á§∫‰ΩøÁî® CIFAR-10 Êï∞ÊçÆÈõÜ ËøõË°åËÆ≠ÁªÉÊàñÂæÆË∞É„ÄÇ
--config.pretrained_dirÔºöÂÆö‰πâÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÊùÉÈáçÁöÑË∑ØÂæÑÔºåËøôÈáåÁõ¥Êé•‰ªé Google Cloud Storage ËØªÂèñÔºåËÄåÊó†ÈúÄÊú¨Âú∞‰∏ãËΩΩ„ÄÇ

**Ë¶ÅÂú® CIFAR-10 Êï∞ÊçÆÈõÜ ‰∏äÂæÆË∞É‰∏Ä‰∏™Âú® ImageNet-21k ‰∏äÈ¢ÑËÆ≠ÁªÉËøáÁöÑ Mixer-B/16 Ê®°Âûã:‰ΩøÁî®Â¶Ç‰∏ãÂëΩ‰ª§üëá**

```bash
python -m vit_jax.main --workdir=/tmp/vit-$(date +%s) \
    --config=$(pwd)/vit_jax/configs/mixer_base16_cifar10.py \
    --config.pretrained_dir='gs://mixer_models/imagenet21k'
```

ËÆ∫Êñá„ÄäHow to train your ViT? ...„Äã‰∏≠Êñ∞Â¢û‰∫ÜË∂ÖËøá 5 ‰∏á‰∏™Ê®°ÂûãÊùÉÈáçÔºàcheckpointsÔºâÁöÑÈ¢ÑËÆ≠ÁªÉÊ®°ÂûãÔºå
‰Ω†ÂèØ‰ª•‰ΩøÁî® [`configs/augreg.py`] ÈÖçÁΩÆÊñá‰ª∂ÂØπËøô‰∫õÊ®°ÂûãËøõË°åÂæÆË∞ÉÔºàfine-tuningÔºâ„ÄÇ
ÂΩì‰Ω†‰ªÖÊåáÂÆöÊ®°ÂûãÂêçÁß∞ ( Âç≥ [`configs/model.py`] ‰∏≠ÁöÑ `config.name`ÂèÇÊï∞ÂÄº)Êó∂, 
Á≥ªÁªü‰ºöËá™Âä®ÈÄâÊã©Âú®‰∏äÊ∏∏È™åËØÅÈõÜ‰∏äÁ≤æÂ∫¶ÊúÄÈ´òÁöÑ ImageNet-21k ÊúÄ‰ºòÊùÉÈáçÔºå ‰πüÂ∞±ÊòØËÆ∫ÊñáÁ¨¨ 4.5 ËäÇ‰∏≠ÊèêÂà∞ÁöÑ‚ÄúÊé®ËçêÔºàrecommendedÔºâ‚ÄùÊ®°Âûã„ÄÇ
Â¶ÇÊûú‰Ω†ÊÉ≥‰∫ÜËß£Âì™ÁßçÊ®°ÂûãÊõ¥ÈÄÇÂêà‰ΩøÁî®ÔºåÂèØ‰ª•ÂèÇËÄÉËÆ∫Êñá‰∏≠ÁöÑ Âõæ 3ÔºàFigure 3Ôºâ„ÄÇ
ÂΩìÁÑ∂Ôºå‰Ω†‰πüÂèØ‰ª•ÊâãÂä®ÊåáÂÆöÂÖ∂‰ªñÈ¢ÑËÆ≠ÁªÉÊùÉÈáçÊñá‰ª∂Ôºå (ÂèÇËÄÉ Colab Á§∫‰æã [`vit_jax_augreg.ipynb`]) 
ÂÖ∑‰ΩìÊñπÊ≥ïÊòØÔºöÂà∞[`gs://vit_models/augreg`] ÁõÆÂΩïÊü•ÊâæÊÉ≥Ë¶ÅÁöÑÊ®°ÂûãÊñá‰ª∂ÂêçÔºàÂéªÊéâ .npz ÂêéÁºÄÔºâÔºåÁÑ∂ÂêéÂú®ÂëΩ‰ª§‰∏≠ÈÄöËøá --config.pretrained_dir ÂèÇÊï∞ÂëäËØâÁ®ãÂ∫èÂä†ËΩΩÂÆÉ„ÄÇ

**ËøêË°å ViT-JAX ‰∏ªËÆ≠ÁªÉËÑöÊú¨ÔºåÊ®°ÂûãÁªìÊûÑÊòØ R_Ti_16ÔºåÊï∞ÊçÆÈõÜÊòØOxford-IIIT Pet üëá**
```bash
python -m vit_jax.main --workdir=/tmp/vit-$(date +%s) \
    --config=$(pwd)/vit_jax/configs/augreg.py:R_Ti_16 \
    --config.dataset=oxford_iiit_pet \
    --config.base_lr=0.01
```
Â¶ÇÊûúËøòË¶ÅÂä†Êåá‰ª§ÂèØ‰ª•Âä† 

Ëá™Â∑±ÊåáÂÆöÁöÑÈ¢ÑËÆ≠ÁªÉÊùÉÈáç --config.pretrained_dir='gs://vit_models/augreg/B_16_i21k_ft1k'ÔºàÊù•Ëá™ gs://vit_models/augreg/ ÁõÆÂΩïÔºâ

ÊâπÈáèÂ§ßÂ∞èÔºö--config.batch_size=256

ËÆ≠ÁªÉÊ≠•Êï∞Ôºö--config.total_steps=20000

ÊùÉÈáçË°∞ÂáèÔºàL2 Ê≠£ÂàôÂåñÔºâÔºö--config.weight_decay=0.0001

ËæìÂá∫Èó¥ÈöîÔºö--config.log_every_steps=100

Â¶ÇÊûúË¶ÅËÆ≠ÁªÉÁõ¥Êé•Êï∞ÊçÆÈõÜÔºåÂèØ‰ª•Âä†‰∏äÈ°πÁõÆ‰∏§Ë°å

--config.dataset=my_dataset #Áî® ImageNet Ê†ºÂºèÂä†ËΩΩÊàëÁöÑËá™ÂÆö‰πâÊï∞ÊçÆÈõÜ

--config.dataset_dir=/home/ws/datasets/my_dataset #Ëá™Â∑±Êï∞ÊçÆÈõÜÁöÑÁõÆÂΩï


ÁõÆÂâçÔºå‰ª£Á†Å‰ºöËá™Âä®‰∏ãËΩΩ CIFAR-10 Âíå CIFAR-100 Êï∞ÊçÆÈõÜ„ÄÇ
‰ªñÂÖ¨ÂÖ±Êï∞ÊçÆÈõÜÊàñËá™ÂÆö‰πâÊï∞ÊçÆÈõÜ‰πüÂèØ‰ª•ÂæàÂÆπÊòìÂú∞ÈõÜÊàêÔºåÂè™ÈúÄ‰ΩøÁî® [tensorflow
datasets library](https://github.com/tensorflow/datasets/). 
ËØ∑Ê≥®ÊÑèÔºåÂ¶ÇÊûú‰Ω†Ê∑ªÂä†‰∫ÜÊñ∞ÁöÑÊï∞ÊçÆÈõÜÔºåËøòÈúÄË¶Å‰øÆÊîπ `vit_jax/input_pipeline.py` Êñá‰ª∂Ôºå‰ª•ÊåáÂÆöËØ•Êï∞ÊçÆÈõÜÁöÑ‰∏Ä‰∫õÁõ∏ÂÖ≥ÂèÇÊï∞ÔºàÂ¶ÇÂõæÂÉèÂ§ßÂ∞è„ÄÅÈÄöÈÅìÊï∞„ÄÅÁ±ªÂà´Êï∞Á≠âÔºâ„ÄÇ

‰ª£Á†ÅÂú®ÂæÆË∞ÉÔºàfine-tuningÔºâÊó∂‰ºöËá™Âä®‰ΩøÁî®ÊâÄÊúâÂèØÁî®ÁöÑ GPU Êàñ TPU„ÄÇ

Ë¶ÅÊü•ÁúãÊâÄÊúâÂèØÁî®ÁöÑÂëΩ‰ª§Ë°åÂèÇÊï∞ÔºàflagsÔºâÔºåÂèØ‰ª•ËøêË°åÔºö `python3 -m vit_jax.train
--help`.

ÂÜÖÂ≠ò‰ΩøÁî®ËØ¥ÊòéÔºö

- ‰∏çÂêåÊ®°ÂûãÂØπÂÜÖÂ≠òÁöÑÈúÄÊ±Ç‰∏çÂêå„ÄÇÂÆûÈôÖÂèØÁî®ÂÜÖÂ≠òËøòÂèñÂÜ≥‰∫éÂä†ÈÄüÂô®ÔºàGPU/TPUÔºâÁöÑÁ±ªÂûãÂíåÊï∞Èáè„ÄÇÂ¶ÇÊûúÈÅáÂà∞ ÊòæÂ≠ò‰∏çË∂≥Ôºàout-of-memory, OOMÔºâ ÈîôËØØÔºåÂèØ‰ª•Ôºö
  Â¢ûÂ§ßÔºàÊ¢ØÂ∫¶Á¥ØÁßØÊ≠•Êï∞Ôºâ`--config.accum_steps=8`Ôºå‰ª•Èôç‰ΩéÂçïÊ≠•ÊòæÂ≠òÂç†Áî®ÊàñÂáèÂ∞è`--config.batch=512` ÔºàÊâπÈáèÂ§ßÂ∞èÔºâ(ÂêåÊó∂Áõ∏Â∫îÂú∞Èôç‰Ωé `--config.base_lr` Â≠¶‰π†Áéá).
- ‰∏ªÊú∫ÔºàhostÔºâÂú®ÂÜÖÂ≠ò‰∏≠‰ºöÁª¥Êä§‰∏Ä‰∏™Êï∞ÊçÆÊâì‰π±ÁºìÂÜ≤Âå∫Ôºàshuffle bufferÔºâ„ÄÇ
  Â¶ÇÊûúÂá∫Áé∞ ‰∏ªÊú∫ÂÜÖÂ≠ò‰∏çË∂≥Ôºàhost OOMÔºâÔºåËÄå‰∏çÊòØÊòæÂç°ÊòæÂ≠ò‰∏çË∂≥ÔºåÂèØ‰ª•ÈÄÇÂΩìÂáèÂ∞èÈªòËÆ§ÁöÑ`--config.shuffle_buffer=50000`ÁöÑÂÄº


## Vision Transformer

‰ΩúËÄÖÔºöAlexey Dosovitskiy*‚Ä†„ÄÅLucas Beyer*„ÄÅAlexander Kolesnikov*„ÄÅDirk Weissenborn*„ÄÅXiaohua Zhai*„ÄÅThomas Unterthiner„ÄÅMostafa Dehghani„ÄÅMatthias Minderer„ÄÅGeorg Heigold„ÄÅSylvain Gelly„ÄÅJakob Uszkoreit Âíå Neil Houlsby*‚Ä†„ÄÇ

Ôºà*ÔºâË°®Á§∫ÊäÄÊúØË¥°ÁåÆÁõ∏ÂêåÔºõÔºà‚Ä†ÔºâË°®Á§∫ÂÖ±ÂêåÊåáÂØº„ÄÇ

![Figure 1 from paper](vit_figure.png)

Ê®°ÂûãÊ¶ÇËø∞Ôºö
Êàë‰ª¨Â∞Ü‰∏ÄÂº†ÂõæÂÉèÂàíÂàÜ‰∏∫**Âõ∫ÂÆöÂ§ßÂ∞èÁöÑÂõæÂÉèÂùóÔºàpatchesÔºâ**Ôºå
ÂØπÊØè‰∏™ÂõæÂÉèÂùóËøõË°å**Á∫øÊÄßÂµåÂÖ•Ôºàlinear embeddingÔºâ**Ôºå
ÂÜç**Âä†ÂÖ•‰ΩçÁΩÆÂµåÂÖ•Ôºàposition embeddingsÔºâ**Ôºå
ÁÑ∂ÂêéÂ∞ÜÂæóÂà∞ÁöÑÂêëÈáèÂ∫èÂàóËæìÂÖ•Âà∞‰∏Ä‰∏™**Ê†áÂáÜÁöÑ Transformer ÁºñÁ†ÅÂô®**‰∏≠„ÄÇ

‰∏∫‰∫ÜÂÆûÁé∞ÂõæÂÉèÂàÜÁ±ªÔºåÊàë‰ª¨ÈááÁî®Ê†áÂáÜÂÅöÊ≥ï ‚Äî‚Äî Âú®ËæìÂÖ•Â∫èÂàóÂâç**Ê∑ªÂä†‰∏Ä‰∏™ÂèØÂ≠¶‰π†ÁöÑ‚ÄúÂàÜÁ±ªÊ†áËÆ∞Ôºàclassification tokenÔºâ‚Äù**Ôºå
Transformer ÊúÄÁªàÈÄöËøáËøô‰∏™Ê†áËÆ∞Êù•ËæìÂá∫Êï¥Âº†ÂõæÂÉèÁöÑÂàÜÁ±ªÁªìÊûú„ÄÇ


### Available ViT models

Êàë‰ª¨Âú®‰∏çÂêåÁöÑ GCSÔºàGoogle Cloud StorageÔºâÂ≠òÂÇ®Ê°∂ ‰∏≠Êèê‰æõ‰∫ÜÂ§öÁßç ViT Ê®°Âûã„ÄÇ
Ëøô‰∫õÊ®°ÂûãÂèØ‰ª•ÈÄöËøáÂ¶Ç‰∏ãÂëΩ‰ª§‰∏ãËΩΩÔºå‰æãÂ¶ÇÔºö

```
wget https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz
```

Ê®°ÂûãÊñá‰ª∂Âêç (ÂéªÊéâ `.npz` ÂêéÁºÄ) ÂØπÂ∫î‰∫é[`vit_jax/configs/models.py`]Êñá‰ª∂‰∏≠ÁöÑ `config.model_name`ÂèÇÊï∞„ÄÇ

Ê®°ÂûãÂ≠òÂÇ®Ë∑ØÂæÑ‰∏éËØ¥ÊòéÔºö
- [`gs://vit_models/imagenet21k`] - Âú® ImageNet-21k Êï∞ÊçÆÈõÜ‰∏äÈ¢ÑËÆ≠ÁªÉÁöÑÊ®°Âûã„ÄÇ.
- [`gs://vit_models/imagenet21k+imagenet2012`] - Âú® ImageNet-21k ‰∏äÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂Âú® ImageNet-2012ÔºàÂç≥Ê†áÂáÜ ImageNetÔºâ‰∏äÂæÆË∞ÉÁöÑÊ®°Âûã„ÄÇ
- [`gs://vit_models/augreg`] - Âú® ImageNet-21k ‰∏äÈ¢ÑËÆ≠ÁªÉÔºåÂπ∂‰ΩøÁî® [AugReg]ÔºàÊï∞ÊçÆÂ¢ûÂº∫‰∏éÊ≠£ÂàôÂåñÔºâ ÊäÄÊúØÁöÑÊ®°ÂûãÔºåÊÄßËÉΩÁõ∏ÊØîÂü∫Á°ÄÁâàÊú¨ÊúâÊòéÊòæÊèêÂçá„ÄÇ
- [`gs://vit_models/sam`] - Âú® ImageNet ‰∏ä‰ΩøÁî® [SAM]ÔºàSharpness-Aware MinimizationÔºåÈîêÂ∫¶ÊÑüÁü•ÊúÄÂ∞èÂåñÔºâ ‰ºòÂåñÊñπÊ≥ïËÆ≠ÁªÉÁöÑÊ®°Âûã„ÄÇ
- [`gs://vit_models/gsam`] - Âú® ImageNet ‰∏ä‰ΩøÁî® [GSAM]ÔºàGeneralized SAMÔºâ ÊñπÊ≥ïËÆ≠ÁªÉÁöÑÊ®°Âûã„ÄÇ

Êàë‰ª¨Êé®Ëçê‰ΩøÁî®‰ª•‰∏ãÈááÁî® [AugReg]ÔºàÊï∞ÊçÆÂ¢ûÂº∫‰∏éÊ≠£ÂàôÂåñÔºâ ÊñπÊ≥ïËÆ≠ÁªÉÁöÑÊ®°ÂûãÊùÉÈáçÔºåËøô‰∫õÊ®°ÂûãÂú®È¢ÑËÆ≠ÁªÉÈò∂ÊÆµÂèñÂæó‰∫ÜÊúÄ‰ºòÁöÑÊÄßËÉΩÊåáÊ†á„ÄÇ
**‰ª•Á¨¨‰∏ÄË°å‰∏∫‰æã**

**L/16 Vision TransformerÔºàViTÔºâÊ®°ÂûãÔºåÈ¢ÑËÆ≠ÁªÉ(Âú® ImageNet-21k Êï∞ÊçÆÈõÜ‰∏äËÆ≠ÁªÉ‰∫Ü 300 ‰∏™ epoch ÁöÑ L/16 Ê®°ÂûãÔºåÂπ∂Â∫îÁî®‰∫Ü Âº∫Êï∞ÊçÆÂ¢ûÂº∫Ôºàaug_strong1Ôºâ„ÄÅL2 ÊùÉÈáçË°∞ÂáèÔºàwd=0.1Ôºâ Á≠âÊäÄÊúØ,Ê®°ÂûãÂ§ßÂ∞èÔºö1243 MiBÔºàÁ∫¶ 1.24 GBÔºâ)ÂíåÂæÆË∞É(Âú® ImageNet-21k ‰∏äÈ¢ÑËÆ≠ÁªÉ ÁöÑÊ®°ÂûãÔºåÂêéÁª≠Âú® ImageNet2012 Êï∞ÊçÆÈõÜ ‰∏äÂæÆË∞É‰∫Ü 20,000 Ê≠•ÔºåÂàÜËæ®Áéá‰∏∫ 384x384ÔºåÂπ∂‰ΩøÁî®‰∫Ü Êõ¥Â∞èÁöÑÂ≠¶‰π†ÁéáÔºàlr=0.01Ôºâ ËøõË°åÂæÆË∞É),Ê®°ÂûãÂú® ImageNet ‰∏äÁöÑÂàÜÁ±ªÊÄßËÉΩÔºàÂáÜÁ°ÆÁéá‰∏∫ 85.59%ÔºâÔºåËØ•Ê®°ÂûãÂ§ÑÁêÜÂõæÂÉèÁöÑÈÄüÂ∫¶‰∏∫ ÊØèÁßí 50 Âº†ÂõæÂÉè„ÄÇ**

|  Model   |                                   Pre-trained checkpoint                                   |   Size   |                                                       Fine-tuned checkpoint                                                        | Resolution | Img/sec | Imagenet accuracy |
| :------- | :----------------------------------------------------------------------------------------- | -------: | :--------------------------------------------------------------------------------------------------------------------------------- | ---------: | ------: | ----------------: |
| L/16     | `gs://vit_models/augreg/L_16-i21k-300ep-lr_0.001-aug_strong1-wd_0.1-do_0.0-sd_0.0.npz`     | 1243 MiB | `gs://vit_models/augreg/L_16-i21k-300ep-lr_0.001-aug_strong1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz`     |        384 |      50 |            85.59% |
| B/16     | `gs://vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0.npz`     |  391 MiB | `gs://vit_models/augreg/B_16-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz`     |        384 |     138 |            85.49% |
| S/16     | `gs://vit_models/augreg/S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0.npz`     |  115 MiB | `gs://vit_models/augreg/S_16-i21k-300ep-lr_0.001-aug_light1-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz`     |        384 |     300 |            83.73% |
| R50+L/32 | `gs://vit_models/augreg/R50_L_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1.npz` | 1337 MiB | `gs://vit_models/augreg/R50_L_32-i21k-300ep-lr_0.001-aug_medium1-wd_0.1-do_0.1-sd_0.1--imagenet2012-steps_20k-lr_0.01-res_384.npz` |        384 |     327 |            85.99% |
| R26+S/32 | `gs://vit_models/augreg/R26_S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0.npz`  |  170 MiB | `gs://vit_models/augreg/R26_S_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz`  |        384 |     560 |            83.85% |
| Ti/16    | `gs://vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0.npz`      |   37 MiB | `gs://vit_models/augreg/Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz`      |        384 |     610 |            78.22% |
| B/32     | `gs://vit_models/augreg/B_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0.npz`      |  398 MiB | `gs://vit_models/augreg/B_32-i21k-300ep-lr_0.001-aug_light1-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz`      |        384 |     955 |            83.59% |
| S/32     | `gs://vit_models/augreg/S_32-i21k-300ep-lr_0.001-aug_none-wd_0.1-do_0.0-sd_0.0.npz`        |  118 MiB | `gs://vit_models/augreg/S_32-i21k-300ep-lr_0.001-aug_none-wd_0.1-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.01-res_384.npz`        |        384 |    2154 |            79.58% |
| R+Ti/16  | `gs://vit_models/augreg/R_Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0.npz`    |   40 MiB | `gs://vit_models/augreg/R_Ti_16-i21k-300ep-lr_0.001-aug_none-wd_0.03-do_0.0-sd_0.0--imagenet2012-steps_20k-lr_0.03-res_384.npz`    |        384 |    2426 |            75.40% |

‰ΩøÁî® [`gs://vit_models/imagenet21k`] Â≠òÂÇ®Ê°∂‰∏≠ÁöÑÊ®°ÂûãÔºåÂ∑≤ÁªèÂ§çÁé∞‰∫ÜÂéüÂßãViTËÆ∫Êñá (https://arxiv.org/abs/2010.11929) ÁªìÊûúÂ¶Ç‰∏ã:

**ËøôÂº†Ë°®Ê†ºÊòæÁ§∫‰∫Ü R50+ViT-B/16 Ê®°ÂûãÂú®‰∏çÂêåÊï∞ÊçÆÈõÜÔºàCIFAR-10„ÄÅCIFAR-100 Âíå ImageNet2012Ôºâ‰∏äÁöÑËÆ≠ÁªÉÊïàÊûúÔºåÂÖ∑‰ΩìÂåÖÊã¨ dropout ÁöÑ‰∏çÂêåËÆæÁΩÆÔºà0.0 Âíå 0.1ÔºâÂØπÊ®°ÂûãÂáÜÁ°ÆÁéáÂíåËÆ≠ÁªÉÊó∂Èó¥ÁöÑÂΩ±Âìç„ÄÇ**

| model        | dataset      | dropout=0.0                                                                                                                                                         | dropout=0.1                                                                                                                                                          |
|:-------------|:-------------|:--------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| R50+ViT-B_16 | cifar10      | 98.72%, 3.9h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5ER50.ViT-B_16/cifar10/do_0.0&_smoothingWeight=0)      | 98.94%, 10.1h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5ER50.ViT-B_16/cifar10/do_0.1&_smoothingWeight=0)      |
| R50+ViT-B_16 | cifar100     | 90.88%, 4.1h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5ER50.ViT-B_16/cifar100/do_0.0&_smoothingWeight=0)     | 92.30%, 10.1h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5ER50.ViT-B_16/cifar100/do_0.1&_smoothingWeight=0)     |
| R50+ViT-B_16 | imagenet2012 | 83.72%, 9.9h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5ER50.ViT-B_16/imagenet2012/do_0.0&_smoothingWeight=0) | 85.08%, 24.2h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5ER50.ViT-B_16/imagenet2012/do_0.1&_smoothingWeight=0) |
| ViT-B_16     | cifar10      | 99.02%, 2.2h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_16/cifar10/do_0.0&_smoothingWeight=0)          | 98.76%, 7.8h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_16/cifar10/do_0.1&_smoothingWeight=0)           |
| ViT-B_16     | cifar100     | 92.06%, 2.2h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_16/cifar100/do_0.0&_smoothingWeight=0)         | 91.92%, 7.8h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_16/cifar100/do_0.1&_smoothingWeight=0)          |
| ViT-B_16     | imagenet2012 | 84.53%, 6.5h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_16/imagenet2012/do_0.0&_smoothingWeight=0)     | 84.12%, 19.3h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_16/imagenet2012/do_0.1&_smoothingWeight=0)     |
| ViT-B_32     | cifar10      | 98.88%, 0.8h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_32/cifar10/do_0.0&_smoothingWeight=0)          | 98.75%, 1.8h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_32/cifar10/do_0.1&_smoothingWeight=0)           |
| ViT-B_32     | cifar100     | 92.31%, 0.8h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_32/cifar100/do_0.0&_smoothingWeight=0)         | 92.05%, 1.8h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_32/cifar100/do_0.1&_smoothingWeight=0)          |
| ViT-B_32     | imagenet2012 | 81.66%, 3.3h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_32/imagenet2012/do_0.0&_smoothingWeight=0)     | 81.31%, 4.9h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-B_32/imagenet2012/do_0.1&_smoothingWeight=0)      |
| ViT-L_16     | cifar10      | 99.13%, 6.9h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_16/cifar10/do_0.0&_smoothingWeight=0)          | 99.14%, 24.7h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_16/cifar10/do_0.1&_smoothingWeight=0)          |
| ViT-L_16     | cifar100     | 92.91%, 7.1h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_16/cifar100/do_0.0&_smoothingWeight=0)         | 93.22%, 24.4h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_16/cifar100/do_0.1&_smoothingWeight=0)         |
| ViT-L_16     | imagenet2012 | 84.47%, 16.8h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_16/imagenet2012/do_0.0&_smoothingWeight=0)    | 85.05%, 59.7h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_16/imagenet2012/do_0.1&_smoothingWeight=0)     |
| ViT-L_32     | cifar10      | 99.06%, 1.9h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_32/cifar10/do_0.0&_smoothingWeight=0)          | 99.09%, 6.1h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_32/cifar10/do_0.1&_smoothingWeight=0)           |
| ViT-L_32     | cifar100     | 93.29%, 1.9h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_32/cifar100/do_0.0&_smoothingWeight=0)         | 93.34%, 6.2h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_32/cifar100/do_0.1&_smoothingWeight=0)          |
| ViT-L_32     | imagenet2012 | 81.89%, 7.5h (A100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_32/imagenet2012/do_0.0&_smoothingWeight=0)     | 81.13%, 15.0h (V100), [tb.dev](https://tensorboard.dev/experiment/nwXQNjudRJW3dtQzhPZwwA/#scalars&regexInput=%5EViT-L_32/imagenet2012/do_0.1&_smoothingWeight=0)     |

Êàë‰ª¨ËøòÂ∏åÊúõÂº∫Ë∞ÉÔºåÈÄöËøáËæÉÁü≠ÁöÑËÆ≠ÁªÉÂë®Êúü‰πüÂèØ‰ª•Ëé∑ÂæóÈ´òË¥®ÈáèÁöÑÁªìÊûúÔºåÂπ∂ÈºìÂä±‰ΩøÁî®Êàë‰ª¨‰ª£Á†ÅÁöÑÁî®Êà∑Ë∞ÉÊï¥Ë∂ÖÂèÇÊï∞Ôºå‰ª•Âú®**ÂáÜÁ°ÆÂ∫¶**Âíå**ËÆ°ÁÆóÈ¢ÑÁÆó**‰πãÈó¥ÊâæÂà∞Âπ≥Ë°°„ÄÇ‰∏ãÈù¢ÁöÑË°®Ê†ºÂ±ïÁ§∫‰∫ÜÂú® CIFAR-10 Âíå CIFAR-100 Êï∞ÊçÆÈõÜ‰∏äÁöÑ‰∏Ä‰∫õÁ§∫‰æã„ÄÇ

| upstream(È¢ÑËÆ≠ÁªÉÊï∞ÊçÆÈõÜÁöÑÊù•Ê∫ê)    | model(Ê®°ÂûãÂêçÁß∞)    | dataset(Êï∞ÊçÆÈõÜ)      | total_steps / warmup_steps(ËÆ≠ÁªÉÁöÑÊÄªÊ≠•Êï∞/ÁÉ≠Ë∫´Ê≠•Êï∞)  | accuracy(ÂàÜÁ±ªÂáÜÁ°ÆÁéá) | wall-clock time(ËÆ≠ÁªÉÁöÑÊÄªÊó∂Èó¥) |                                                                         link(TensorBoardÂèØËßÜÂåñÁªìÊûú) |
| ----------- | -------- | ------------ | --------------------------- | -------- | --------------- | ---------------------------------------------------------------------------- |
| imagenet21k | ViT-B_16 | cifar10      | 500 / 50                    |   98.59% |             17m | [tensorboard.dev](https://tensorboard.dev/experiment/QgkpiW53RPmjkabe1ME31g/) |
| imagenet21k | ViT-B_16 | cifar10      | 1000 / 100                  |   98.86% |             39m | [tensorboard.dev](https://tensorboard.dev/experiment/w8DQkDeJTOqJW5js80gOQg/) |
| imagenet21k | ViT-B_16 | cifar100     | 500 / 50                    |   89.17% |             17m | [tensorboard.dev](https://tensorboard.dev/experiment/5hM4GrnAR0KEZg725Ewnqg/) |
| imagenet21k | ViT-B_16 | cifar100     | 1000 / 100                  |   91.15% |             39m | [tensorboard.dev](https://tensorboard.dev/experiment/QLQTaaIoT9uEcAjtA0eRwg/) |


## MLP-Mixer

‰ΩúËÄÖÔºöIlya Tolstikhin*„ÄÅNeil Houlsby*„ÄÅAlexander Kolesnikov*„ÄÅLucas Beyer*„ÄÅ
Xiaohua Zhai„ÄÅThomas Unterthiner„ÄÅJessica Yung„ÄÅAndreas Steiner„ÄÅDaniel Keysers„ÄÅ
Jakob Uszkoreit„ÄÅMario Lucic„ÄÅAlexey Dosovitskiy„ÄÇ

Ôºà*ÔºâË°®Á§∫ÊäÄÊúØË¥°ÁåÆÁõ∏Âêå„ÄÇ

![Figure 1 from paper](mixer_figure.png)

MLP-MixerÔºàÁÆÄÁß∞ MixerÔºâÁî±ÊØè‰∏™ÂõæÂÉèÂùóÁöÑÁ∫øÊÄßÂµåÂÖ•Ôºàper-patch linear embeddingsÔºâ„ÄÅMixer Â±ÇÂíåÂàÜÁ±ªÂ§¥Ôºàclassifier headÔºâÁªÑÊàê„ÄÇ
Mixer Â±ÇÂåÖÂê´‰∏Ä‰∏™ token-mixing MLP Âíå‰∏Ä‰∏™ channel-mixing MLPÔºåÊØè‰∏™ MLP Áî±‰∏§Â±ÇÂÖ®ËøûÊé•Â±ÇÂíå‰∏Ä‰∏™ GELU ÈùûÁ∫øÊÄßÊøÄÊ¥ªÂáΩÊï∞ÁªÑÊàê„ÄÇ
ÂÖ∂‰ªñÁªÑÊàêÈÉ®ÂàÜÂåÖÊã¨ÔºöË∑≥Ë∑ÉËøûÊé•Ôºàskip-connectionsÔºâ„ÄÅdropout Âíå Á∫øÊÄßÂàÜÁ±ªÂ§¥Ôºàlinear classifier headÔºâ„ÄÇ

ÂÆâË£ÖÊ≠•È™§ËØ∑ÂèÇËÄÉ‰∏äÈù¢ÁöÑ [the same steps](#installation)

### Available Mixer models

Êàë‰ª¨Êèê‰æõ‰∫ÜÂú® ImageNet Âíå ImageNet-21k Êï∞ÊçÆÈõÜ‰∏äÈ¢ÑËÆ≠ÁªÉÁöÑ Mixer-B/16 Âíå Mixer-L/16 Ê®°Âûã„ÄÇ
ËØ¶ÁªÜ‰ø°ÊÅØÂèØ‰ª•Âú® Mixer ËÆ∫ÊñáÁöÑÁ¨¨ 3 Ë°® ‰∏≠ÊâæÂà∞„ÄÇ
ÊâÄÊúâÊ®°ÂûãÂèØ‰ª•Âú®‰ª•‰∏ãÈìæÊé•‰∏ãËΩΩÔºö

https://console.cloud.google.com/storage/mixer_models/

ËØ∑Ê≥®ÊÑèÔºåËøô‰∫õÊ®°Âûã‰πüÂèØ‰ª•Áõ¥Êé•‰ªé TF-Hub Ëé∑Âèñ:
[sayakpaul/collections/mlp-mixer] (Áî± [Sayak
Paul]Êèê‰æõÁöÑÂ§ñÈÉ®Ë¥°ÁåÆ).

[sayakpaul/collections/mlp-mixer]: https://tfhub.dev/sayakpaul/collections/mlp-mixer

### Expected Mixer results

Êàë‰ª¨Âú® Google Cloud ÁöÑÂõõ‰∏™ V100 GPU Êú∫Âô®‰∏äËøêË°å‰∫ÜÂæÆË∞É‰ª£Á†ÅÔºå‰ΩøÁî®‰∫ÜËØ•‰ªìÂ∫ì‰∏≠ÁöÑÈªòËÆ§ÈÄÇÈÖçÂèÇÊï∞„ÄÇ‰ª•‰∏ãÊòØÁªìÊûúÔºö

upstream     | model      | dataset | accuracy | wall_clock_time | link
:----------- | :--------- | :------ | -------: | :-------------- | :---
ImageNet     | Mixer-B/16 | cifar10 | 96.72%   | 3.0h            | [tensorboard.dev](https://tensorboard.dev/experiment/j9zCYt9yQVm93nqnsDZayA/)
ImageNet     | Mixer-L/16 | cifar10 | 96.59%   | 3.0h            | [tensorboard.dev](https://tensorboard.dev/experiment/Q4feeErzRGGop5XzAvYj2g/)
ImageNet-21k | Mixer-B/16 | cifar10 | 96.82%   | 9.6h            | [tensorboard.dev](https://tensorboard.dev/experiment/mvP4McV2SEGFeIww20ie5Q/)
ImageNet-21k | Mixer-L/16 | cifar10 | 98.34%   | 10.0h           | [tensorboard.dev](https://tensorboard.dev/experiment/dolAJyQYTYmudytjalF6Jg/)


## LiT models

ÊúâÂÖ≥ËØ¶ÁªÜ‰ø°ÊÅØÔºåËØ∑ÂèÇËÄÉ Google AI ÂçöÂÆ¢ÊñáÁ´†
[LiT: adding language understanding to image models](http://ai.googleblog.com/2022/04/locked-image-tuning-adding-language.html),
ÊàñÈòÖËØª CVPR ËÆ∫Êñá "LiT: Zero-Shot Transfer with Locked-image text Tuning"
(https://arxiv.org/abs/2111.07991).

Êàë‰ª¨ÂèëÂ∏É‰∫Ü‰∏Ä‰∏™ Transformer B/16-base Ê®°ÂûãÔºåÂÖ∑Êúâ 72.1% ÁöÑ ImageNet Èõ∂-shot ÂáÜÁ°ÆÁéáÔºå
‰ª•Âèä‰∏Ä‰∏™ L/16-large Ê®°ÂûãÔºåÂÖ∑Êúâ 75.7% ÁöÑ ImageNet Èõ∂-shot ÂáÜÁ°ÆÁéá„ÄÇ
ÊúâÂÖ≥Ëøô‰∫õÊ®°ÂûãÁöÑÊõ¥Â§öËØ¶ÊÉÖÔºåËØ∑ÂèÇÈòÖ[LiT model card](model_cards/lit.md).

Êàë‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™ÊµèËßàÂô®ÂÜÖÁöÑÊºîÁ§∫Ôºå‰ΩøÁî®‰∫ÜÂ∞èÂûãÊñáÊú¨ÁºñÁ†ÅÂô®Ôºå‰æõ‰∫§‰∫íÂºè‰ΩøÁî®ÔºàÊúÄÂ∞èÁöÑÊ®°ÂûãÁîöËá≥ÂèØ‰ª•Âú®Áé∞‰ª£ÊâãÊú∫‰∏äËøêË°åÔºâ:

https://google-research.github.io/vision_transformer/lit/

ÊúÄÂêéÔºåÊàë‰ª¨Êèê‰æõ‰∫Ü‰∏Ä‰∏™ Colab Á§∫‰æãÔºåÂ±ïÁ§∫Â¶Ç‰Ωï‰ΩøÁî® JAX Ê®°ÂûãÔºåÁªìÂêàÂõæÂÉèÂíåÊñáÊú¨ÁºñÁ†ÅÂô®Ôºö

https://colab.research.google.com/github/google-research/vision_transformer/blob/main/lit.ipynb

ËØ∑Ê≥®ÊÑèÔºå‰ª•‰∏äÊ®°ÂûãÂ∞ö‰∏çÊîØÊåÅÂ§öËØ≠Ë®ÄËæìÂÖ•Ôºå‰ΩÜÊàë‰ª¨Ê≠£Âú®Âä™ÂäõÂèëÂ∏ÉÊ≠§Á±ªÊ®°ÂûãÔºåÂπ∂Â∞ÜÂú®ÂÆÉ‰ª¨ÂèØÁî®Êó∂Êõ¥Êñ∞Êú¨‰ªìÂ∫ì„ÄÇ

Êú¨‰ªìÂ∫ì‰ªÖÂåÖÂê´ LiT Ê®°ÂûãÁöÑËØÑ‰º∞‰ª£Á†Å„ÄÇËÆ≠ÁªÉ‰ª£Á†ÅÂèØ‰ª•Âú® `big_vision` ‰ªìÂ∫ì‰∏≠ÊâæÂà∞Ôºö

https://github.com/google-research/big_vision/tree/main/big_vision/configs/proj/image_text

È¢ÑËÆ°ÁöÑÈõ∂-shot ÁªìÊûúÂèØ‰ª•Âú® [`model_cards/lit.md`] ‰∏≠ÊâæÂà∞ÔºàËØ∑Ê≥®ÊÑèÔºåÈõ∂-shot ËØÑ‰º∞‰∏é Colab ‰∏≠ÁÆÄÂåñÁöÑËØÑ‰º∞Áï•Êúâ‰∏çÂêåÔºâÔºö

**Èõ∂-shot ÂáÜÁ°ÆÁéáÔºöËØ•Ê®°ÂûãÂú®‰∏çÂêå‰ªªÂä°ÔºàÂ¶ÇÂàÜÁ±ª„ÄÅÊ£ÄÁ¥¢Ôºâ‰∏ä‰∏çÈúÄË¶ÅÈ¢ùÂ§ñÁöÑËÆ≠ÁªÉÂ∞±ËÉΩËææÂà∞ÁöÑÂáÜÁ°ÆÁéá„ÄÇ**

| Model | B16B_2 | L16L |
| :--- | ---: | ---: |
| ImageNet zero-shot | 73.9% | 75.7% |
| ImageNet v2 zero-shot | 65.1% | 66.6% |
| CIFAR100 zero-shot | 79.0% | 80.5% |
| Pets37 zero-shot | 83.3% | 83.3% |
| Resisc45 zero-shot | 25.3% | 25.6% |
| MS-COCO Captions image-to-text retrieval | 51.6% | 48.5% |
| MS-COCO Captions text-to-image retrieval | 31.8% | 31.1% |

## Running on cloud

ËôΩÁÑ∂‰∏äÈù¢ÁöÑ [colabs](#colab) ÈùûÂ∏∏ÈÄÇÂêàÂÖ•Èó®Ôºå‰ΩÜÈÄöÂ∏∏‰Ω†ÂèØËÉΩÂ∏åÊúõÂú®Êõ¥Âº∫Â§ßÁöÑÊú∫Âô®‰∏äËøõË°åËÆ≠ÁªÉÔºå‰ΩøÁî®Êõ¥Â§öÁöÑÂä†ÈÄüÂô®ÔºàÂ¶Ç GPU Êàñ TPUÔºâ„ÄÇ

### Create a VM

‰Ω†ÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§Âú® Google Cloud ‰∏äËÆæÁΩÆ‰∏Ä‰∏™Â∏¶ GPU ÁöÑËôöÊãüÊú∫ÔºàVMÔºâÔºö

**Êèê‰æõ‰∫ÜÂ¶Ç‰ΩïÂú® Google Cloud ‰∏äËÆæÁΩÆÂπ∂ÁÆ°ÁêÜ‰∏Ä‰∏™ GPU ËôöÊãüÊú∫ÁöÑËØ¶ÁªÜÊ≠•È™§ÔºåÂëΩ‰ª§Â¶Ç‰∏ã:**

```bash
# ËÆæÁΩÆÊâÄÊúâÂëΩ‰ª§‰ΩøÁî®ÁöÑÂèòÈáè„ÄÇ
# ËØ∑Ê≥®ÊÑèÔºåÈ°πÁõÆÂøÖÈ°ªÂ∑≤ÂêØÁî®Ë¥¶Âçï„ÄÇ
# ÊúâÂÖ≥Â∏¶ GPU ÁöÑÂå∫ÂüüÂàóË°®ÔºåËØ∑ÂèÇËÄÉ
# https://cloud.google.com/compute/docs/gpus/gpu-regions-zones
PROJECT=my-awesome-gcp-project  # Project must have billing enabled. # È°πÁõÆÂøÖÈ°ªÂêØÁî®Ë¥¶Âçï„ÄÇ
VM_NAME=vit-jax-vm-gpu
ZONE=europe-west4-b

# ‰ª•‰∏ãËÆæÁΩÆÂ∑≤ÈÄöËøáËØ•‰ªìÂ∫ìËøõË°åÊµãËØï„ÄÇ‰Ω†ÂèØ‰ª•ÈÄâÊã©ÂÖ∂‰ªñ
# ÈïúÂÉèÂíåÊú∫Âô®Á±ªÂûãÁöÑÁªÑÂêàÔºà‰æãÂ¶ÇÔºâÔºåÂèÇËÄÉ‰ª•‰∏ã gcloud ÂëΩ‰ª§Ôºö
# gcloud compute images list --project ml-images
# gcloud compute machine-types list
# Á≠âÁ≠â„ÄÇ
gcloud compute instances create $VM_NAME \
    --project=$PROJECT --zone=$ZONE \
    --image=c1-deeplearning-tf-2-5-cu110-v20210527-debian-10 \
    --image-project=ml-images --machine-type=n1-standard-96 \
    --scopes=cloud-platform,storage-full --boot-disk-size=256GB \
    --boot-disk-type=pd-ssd --metadata=install-nvidia-driver=True \
    --maintenance-policy=TERMINATE \
    --accelerator=type=nvidia-tesla-v100,count=8

# Âú®ËÆæÁΩÆÂπ∂ÂêØÂä®ËôöÊãüÊú∫Âá†ÂàÜÈíüÂêéÔºåËøûÊé•Âà∞ËôöÊãüÊú∫„ÄÇ
gcloud compute ssh --project $PROJECT --zone $ZONE $VM_NAME

# ‰ΩøÁî®ÂêéÂÅúÊ≠¢ËôöÊãüÊú∫ÔºàÂÅúÊ≠¢ÁöÑËôöÊãüÊú∫Âè™‰ºö‰∫ßÁîüÂ≠òÂÇ®Ë¥πÁî®Ôºâ„ÄÇ
gcloud compute instances stop --project $PROJECT --zone $ZONE $VM_NAME

# ‰ΩøÁî®ÂêéÂà†Èô§ËôöÊãüÊú∫ÔºàËøôÂ∞ÜÂà†Èô§ËôöÊãüÊú∫‰∏äÂ≠òÂÇ®ÁöÑÊâÄÊúâÊï∞ÊçÆÔºâ„ÄÇ
gcloud compute instances delete --project $PROJECT --zone $ZONE $VM_NAME
```

ÂèØ‰ª•‰ΩøÁî®‰ª•‰∏ãÁ±ª‰ººÁöÑÂëΩ‰ª§Êù•ÂàõÂª∫‰∏Ä‰∏™Â∏¶ TPU ÁöÑ‰∫ëËôöÊãüÊú∫ÔºàVMÔºâ„ÄÇ‰∏ãÈù¢ÁöÑÂëΩ‰ª§Êù•Ëá™TPUÊïôÁ®ã [TPU tutorial]):

[TPU tutorial]: https://cloud.google.com/tpu/docs/jax-quickstart-tpu-vm

```bash
PROJECT=my-awesome-gcp-project  # Project must have billing enabled.
VM_NAME=vit-jax-vm-tpu
ZONE=europe-west4-a

# ÂàùÂßãËÆæÁΩÆÊó∂ÈúÄË¶ÅÂàõÂª∫ÊúçÂä°Ë∫´‰ªΩ„ÄÇ
gcloud beta services identity create --service tpu.googleapis.com

# ÂàõÂª∫‰∏Ä‰∏™Áõ¥Êé•ËøûÊé• TPU ÁöÑËôöÊãüÊú∫„ÄÇ
gcloud alpha compute tpus tpu-vm create $VM_NAME \
    --project=$PROJECT --zone=$ZONE \
    --accelerator-type v3-8 \
    --version tpu-vm-base

# ËøûÊé•Âà∞ËôöÊãüÊú∫ÔºàËÆæÁΩÆÂíåÂêØÂä®Êú∫Âô®ÈúÄË¶Å‰∏Ä‰∫õÊó∂Èó¥Ôºâ„ÄÇ
gcloud alpha compute tpus tpu-vm ssh --project $PROJECT --zone $ZONE $VM_NAME

# ‰ΩøÁî®ÂêéÂÅúÊ≠¢ËôöÊãüÊú∫ÔºàÂÅúÊ≠¢ÂêéÁöÑËôöÊãüÊú∫Âè™‰ºö‰∫ßÁîüÂ≠òÂÇ®Ë¥πÁî®Ôºâ„ÄÇ
gcloud alpha compute tpus tpu-vm stop --project $PROJECT --zone $ZONE $VM_NAME

# ‰ΩøÁî®ÂêéÂà†Èô§ËôöÊãüÊú∫ÔºàËøôÂ∞ÜÂà†Èô§ËôöÊãüÊú∫‰∏äÂ≠òÂÇ®ÁöÑÊâÄÊúâÊï∞ÊçÆÔºâ„ÄÇ
gcloud alpha compute tpus tpu-vm delete --project $PROJECT --zone $ZONE $VM_NAME
```

### Setup VM

ÁÑ∂ÂêéÔºå‰Ω†ÂèØ‰ª•ÂÉèÂæÄÂ∏∏‰∏ÄÊ†∑Ëé∑Âèñ‰ªìÂ∫ìÂπ∂ÂÆâË£Ö‰æùËµñ (ÂåÖÊã¨Â∏¶Êúâ TPU ÊîØÊåÅÁöÑ `jaxlib`) :

```bash
git clone --depth=1 --branch=master https://github.com/google-research/vision_transformer
cd vision_transformer

# optional: install virtualenv
pip3 install virtualenv
python3 -m virtualenv env
. env/bin/activate
```

Â¶ÇÊûú‰Ω†ËøûÊé•Âà∞Â∏¶Êúâ GPU ÁöÑËôöÊãüÊú∫Ôºå‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ÂÆâË£Ö JAX ÂíåÂÖ∂‰ªñ‰æùËµñÔºö

```bash
pip install -r vit_jax/requirements.txt
```

Â¶ÇÊûú‰Ω†ËøûÊé•Âà∞Â∏¶Êúâ TPU ÁöÑËôöÊãüÊú∫Ôºå‰ΩøÁî®‰ª•‰∏ãÂëΩ‰ª§ÂÆâË£Ö JAX ÂíåÂÖ∂‰ªñ‰æùËµñÔºö

```bash
pip install -r vit_jax/requirements-tpu.txt
```

ÂÆâË£Ö [Flaxformer](https://github.com/google/flaxformer), Âπ∂ÊåâÁÖßÁõ∏Â∫î‰ªìÂ∫ì‰∏≠ÁöÑÂÆâË£ÖËØ¥ÊòéËøõË°åÊìç‰Ωú„ÄÇ

ÂØπ‰∫é GPU Âíå TPUÔºåÂèØ‰ª•ÈÄöËøá‰ª•‰∏ãÂëΩ‰ª§Ê£ÄÊü• JAX ÊòØÂê¶ËÉΩËøûÊé•Âà∞Â∑≤ÈôÑÂä†ÁöÑÂä†ÈÄüÂô®Ôºö
```bash
python -c 'import jax; print(jax.devices())'
```

ÊúÄÂêéÔºåÊâßË°å[fine-tuning a model](#fine-tuning-a-model)ÈÉ®ÂàÜÊèêÂà∞ÁöÑÂëΩ‰ª§


## Bibtex

**ÂºïÁî®ËÆ∫Êñá**

```
@article{dosovitskiy2020vit,
  title={An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and  Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and Uszkoreit, Jakob and Houlsby, Neil},
  journal={ICLR},
  year={2021}
}

@article{tolstikhin2021mixer,
  title={MLP-Mixer: An all-MLP Architecture for Vision},
  author={Tolstikhin, Ilya and Houlsby, Neil and Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Unterthiner, Thomas and Yung, Jessica and Steiner, Andreas and Keysers, Daniel and Uszkoreit, Jakob and Lucic, Mario and Dosovitskiy, Alexey},
  journal={arXiv preprint arXiv:2105.01601},
  year={2021}
}

@article{steiner2021augreg,
  title={How to train your ViT? Data, Augmentation, and Regularization in Vision Transformers},
  author={Steiner, Andreas and Kolesnikov, Alexander and and Zhai, Xiaohua and Wightman, Ross and Uszkoreit, Jakob and Beyer, Lucas},
  journal={arXiv preprint arXiv:2106.10270},
  year={2021}
}

@article{chen2021outperform,
  title={When Vision Transformers Outperform ResNets without Pretraining or Strong Data Augmentations},
  author={Chen, Xiangning and Hsieh, Cho-Jui and Gong, Boqing},
  journal={arXiv preprint arXiv:2106.01548},
  year={2021},
}

@article{zhuang2022gsam,
  title={Surrogate Gap Minimization Improves Sharpness-Aware Training},
  author={Zhuang, Juntang and Gong, Boqing and Yuan, Liangzhe and Cui, Yin and Adam, Hartwig and Dvornek, Nicha and Tatikonda, Sekhar and Duncan, James and Liu, Ting},
  journal={ICLR},
  year={2022},
}

@article{zhai2022lit,
  title={LiT: Zero-Shot Transfer with Locked-image Text Tuning},
  author={Zhai, Xiaohua and Wang, Xiao and Mustafa, Basil and Steiner, Andreas and Keysers, Daniel and Kolesnikov, Alexander and Beyer, Lucas},
  journal={CVPR},
  year={2022}
}
```


## Changelog

In reverse chronological order:

- 2022-08-18: Added LiT-B16B_2 model that was trained for 60k steps
  (LiT_B16B: 30k) without linear head on the image side (LiT_B16B: 768) and has
  better performance.

- 2022-06-09: Added the ViT and Mixer models trained from scratch using
  [GSAM] on ImageNet without strong data augmentations. The resultant ViTs
  outperform those of similar sizes trained using AdamW optimizer or the
  original [SAM] algorithm, or with strong data augmentations.

- 2022-04-14: Added models and Colab for [LiT models](#lit-models).

- 2021-07-29: Added ViT-B/8 AugReg models (3 upstream checkpoints and adaptations
  with resolution=224).

- 2021-07-02: Added the "When Vision Transformers Outperform
  ResNets..." paper

- 2021-07-02: Added [SAM](https://arxiv.org/abs/2010.01412)
  (Sharpness-Aware Minimization) optimized ViT and MLP-Mixer checkpoints.

- 2021-06-20: Added the "How to train your ViT? ..." paper, and a new
  Colab to explore the >50k pre-trained and fine-tuned checkpoints mentioned in
  the paper.

- 2021-06-18: This repository was rewritten to use Flax Linen API and
  `ml_collections.ConfigDict` for configuration.

- 2021-05-19: With publication of the "How to train your ViT? ..."
  paper, we added more than 50k ViT and hybrid models pre-trained on ImageNet and
  ImageNet-21k with various degrees of data augmentation and model regularization,
  and fine-tuned on ImageNet, Pets37, Kitti-distance, CIFAR-100, and Resisc45.
  Check out [`vit_jax_augreg.ipynb`] to navigate this treasure trove of models!
  For example, you can use that Colab to fetch the filenames of recommended
  pre-trained and fine-tuned checkpoints from the `i21k_300` column of Table 3 in
  the paper.

- 2020-12-01: Added the R50+ViT-B/16 hybrid model (ViT-B/16 on
  top of a Resnet-50 backbone). When pretrained on imagenet21k, this model
  achieves almost the performance of the L/16 model with less than half the
  computational finetuning cost. Note that "R50" is somewhat modified for the
  B/16 variant: The original ResNet-50 has [3,4,6,3] blocks, each reducing the
  resolution of the image by a factor of two. In combination with the ResNet
  stem this would result in a reduction of 32x so even with a patch size of
  (1,1) the ViT-B/16 variant cannot be realized anymore. For this reason we
  instead use [3,4,9] blocks for the R50+B/16 variant.

- 2020-11-09: Added the ViT-L/16 model.

- 2020-10-29: Added ViT-B/16 and ViT-L/16 models pretrained
  on ImageNet-21k and then fine-tuned on ImageNet at 224x224 resolution (instead
  of default 384x384). These models have the suffix "-224" in their name.
  They are expected to achieve 81.2% and 82.7% top-1 accuracies respectively.


## Disclaimers

Open source release prepared by Andreas Steiner.

Ê≥®ÊÑèÔºöÊú¨‰ªìÂ∫ìÊòØ‰ªé
[google-research/big_transfer](https://github.com/google-research/big_transfer)ÂàÜÂèâÂπ∂‰øÆÊîπËÄåÊù•ÁöÑ„ÄÇ

**Ëøô‰∏çÊòØ‰∏Ä‰∏™ÂÆòÊñπÁöÑ Google ‰∫ßÂìÅ„ÄÇ**


[GSAM]: https://arxiv.org/abs/2203.08065
[SAM]: https://arxiv.org/abs/2010.01412
[AugReg]: https://arxiv.org/abs/2106.10270

[`vit_jax/configs/models.py`]: https://github.com/google-research/vision_transformer/blob/main/vit_jax/configs/models.py
[`model_cards/lit.md`]: https://github.com/google-research/vision_transformer/blob/main/model_cards/lit.md

[`configs/augreg.py`]: https://github.com/google-research/vision_transformer/blob/main/vit_jax/configs/augreg.py
[`configs/model.py`]: https://github.com/google-research/vision_transformer/blob/main/vit_jax/configs/models.py
[`vit_jax_augreg.ipynb`]: https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax_augreg.ipynb
[`vit_jax.ipynb`]: https://colab.research.google.com/github/google-research/vision_transformer/blob/main/vit_jax.ipynb

[`gs://vit_models/imagenet21k`]: https://console.cloud.google.com/storage/browser/vit_models/imagenet21k/
[`gs://vit_models/imagenet21k+imagenet2012`]: https://console.cloud.google.com/storage/browser/vit_models/imagenet21k+imagenet2012/
[`gs://vit_models/augreg`]: https://console.cloud.google.com/storage/browser/vit_models/augreg/
[`gs://vit_models/sam`]: https://console.cloud.google.com/storage/browser/vit_models/sam/
[`gs://mixer_models/sam`]: https://console.cloud.google.com/storage/mixer_models/sam/
[`gs://vit_models/gsam`]: https://console.cloud.google.com/storage/browser/vit_models/gsam/
[`gs://mixer_models/gsam`]: https://console.cloud.google.com/storage/mixer_models/gsam/
